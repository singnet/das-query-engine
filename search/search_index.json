{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This package is a query engine API for Distributed Atomspace (DAS).</p>"},{"location":"Deployment-Process/","title":"Deployment Process","text":""},{"location":"Deployment-Process/#new-version-of-das-atomdb","title":"New Version of DAS AtomDB","text":"<ol> <li>Go to the repository at AtomDB repository.</li> </ol> <p>To publish a new version of DAS AtomDB, the first step is to access the AtomDB repository.</p> <p></p> <ol> <li>Update the version parameter in pyproject.toml</li> </ol> <p>Before starting to publish the version, it is crucial to ensure that the pyproject.toml file is updated with the number of the desired new version, locating and changing the version parameter in the [tool.poetry] section.</p> <p></p> <ol> <li>Commit the changes</li> </ol> <p>After this change, it is necessary to commit to the master branch to record the change.</p> <p></p> <ol> <li>Verify latest created tag before versioning</li> </ol> <p>It is important to note what the last version created was at https://github.com/singnet/das-atom-db/tags before creating a new version.</p> <ol> <li>Execute PyPI Publishing Workflow     Navigate to Actions, select the \"Publish to PyPI\" workflow, and click on \"Run workflow.\" Choose the master branch, enter the version number in the form that appears (format: 1.0.0) ensure that the inserted version matches the one previously added in the pyproject.toml file, and click the \"Run workflow\" button.</li> </ol> <p></p> <ol> <li>Monitor the workflow execution. Ensure all jobs complete successfully.</li> </ol> <p>After the workflow execution, refresh the page and check if a new workflow is running. By clicking on it, you can track all jobs. At the end of the process, all jobs should have a green check mark. If there is an error in any job, it is possible to click on it to view the logs and identify the cause of the problem.</p> <p></p> <ol> <li>Verify the new tag at DAS AtomDB repository tags and pypi project history.</li> </ol> <p>If everything goes as expected, the new version tag should be available at https://github.com/singnet/das-atom-db/tags and https://pypi.org/project/hyperon-das-atomdb/#history.</p>"},{"location":"Deployment-Process/#new-version-of-das-query-engine","title":"New Version of DAS Query Engine","text":"<ol> <li>Go to the repository at the Query Engine repository.</li> </ol> <p>To publish a new version of DAS Query Engine, follow a process similarto the one described above for Das AtomDB. Access the repository athttps://github.com/singnet/das-query-engine</p> <p></p> <ol> <li>Update the version parameter in pyproject.toml</li> </ol> <p>Make sure to update the version number in the pyproject.toml file. Additionally, it is necessary to update the version of hyperon-das-atomdb in the dependencies, as specified in the [tool.poetry.dependencies] section.</p> <p></p> <ol> <li>Commit this change to the master branch.</li> </ol> <p>After this change, it is necessary to commit to the master branch to record the change.</p> <p></p> <ol> <li>Verify latest created tag before versioning</li> </ol> <p>It is important to note what the last version created was at https://github.com/singnet/das-query-engine/tags before creating a new version.</p> <ol> <li>Execute PyPI Publishing Workflow</li> </ol> <p>Initiate the 'Publish to PyPI' Workflow Manually via the 'Actions' Tab in the Repository. Click 'Run workflow' and proceed with the provided instructions, ensuring the master branch is selected. Enter the desired version number in the format 1.0.0, then click 'Run workflow' to proceed.</p> <p></p> <ol> <li>Monitor the workflow execution. Ensure all jobs complete successfully.</li> </ol> <p>Just like in the case of DAS AtomDB, refresh the page and check if a new workflow is running. By clicking on it, you can track all jobs. At the end of the process, all jobs should have a green check mark. If there is an error in any job, it is possible to click on it to view the logs and identify the cause of the problem.</p> <ol> <li>Verify the new tag at DAS Query Engine repository tags and pypi project history.</li> </ol> <p>If everything goes as expected, the new version tag should be available at https://github.com/singnet/das-query-engine/tags and https://pypi.org/project/hyperon-das/#history.</p>"},{"location":"Deployment-Process/#new-version-of-das-serverless-functions","title":"New Version of DAS Serverless Functions","text":"<ol> <li>Navigate to the DAS Serverless Functions repository.</li> </ol> <ol> <li>Update the hyperon-das version in the requirements file</li> </ol> <p>Update the version of the hyperon-das in the das-query-engine/requirements.txt file. This ensures that the correct version is used during the workflow build.</p> <p></p> <ol> <li>Commit the change to the master branch.</li> </ol> <p>After this change, it is necessary to commit to the master branch to record the change.</p> <p></p> <ol> <li>Verify latest created tag before versioning</li> </ol> <p>It is important to note what the last version created was at https://github.com/singnet/das-serverless-functions/tags before creating a new version.</p> <ol> <li>Select the \"Vultr Build\" workflow in Actions and run it manually.</li> </ol> <p>Manually trigger the 'Vultr Build' workflow via the 'Actions' tab in the repository. Ensure the master branch is selected, then input the desired version number following the format 1.0.0. Next, choose 'das-query-engine' from the dropdown menu, and finally, click 'Run workflow' to proceed.</p> <p></p> <ol> <li>Monitor the workflow execution. Ensure all jobs complete successfully.</li> </ol> <p>After the workflow execution, refresh the page and check if a new workflow is running. By clicking on it, you can track all jobs. At the end of the process, all jobs should have a green check mark. If there is an error in any job, it is possible to click on it to view the logs and identify the cause of the problem.</p> <p></p> <ol> <li>Verify the new tag at DAS Serverless Functions repository tags and Docker Hub.</li> </ol> <p>It is important to note that this pipeline should generate an img on Docker Hub, following the format 1.0.0-queryengine. Make sure that the img is generated correctly and available at https://hub.docker.com/r/trueagi/das/tags. After the workflow execution, verify if all jobs were successfully completed. The new version tag should be available at https://github.com/singnet/das-serverless-functions/tags.</p>"},{"location":"Deployment-Process/#deploying-the-built-image-to-production-and-development-environments","title":"Deploying the Built Image to Production and Development Environments","text":"<ol> <li>For deployment, navigate to DAS Infra Stack Vultr repository.</li> </ol> <p>The publication process of the img generated in the production and development environments is carried out in the das-infra-stack-vultr repository.</p> <p></p> <ol> <li>Update requirements.txt and das-function.yml.</li> </ol> <p>Before starting the deployment, it is necessary to update the version of hyperon-das in the requirements.txt file, ensuring that the correct version is used during integration tests. Before committing the changes to a branch, make the necessary changes in the das-function.yml file, updating the image version to the one generated earlier.</p> <p></p> <p></p> <ol> <li>Commit/Merge the changes to the develop branch.</li> </ol> <p>Commit your changes to the 'develop' branch or merge them into the 'develop' branch for deployment to the development environment. Following the merge, the 'Vultr Deployment' pipeline will initiate automatically. Verify the successful completion of all jobs with the 'develop' suffix to ensure the development environment is accurately updated.</p> <p></p> <ol> <li>Merge develop branch into master.</li> </ol> <p>After verification, make a PR from develop to master. After the merge to master, check if all jobs were successfully completed, ensuring that the production environment is correctly updated. If errors occur during tests, they are likely related to the response format, which may have been changed due to previously published libraries. In case of problems, it is possible to rollback the version by reverting the commit to return to the previous version.</p> <p></p>"},{"location":"Deployment-Process/#new-version-of-das-metta-parser","title":"New Version of DAS Metta Parser","text":"<ol> <li>Go to the repository at the DAS Metta Parser repository.</li> </ol> <p>To publish a new version of DAS Metta Parser, access the repository athttps://github.com/singnet/das-metta-parser</p> <p></p> <ol> <li>Verify latest created tag before versioning</li> </ol> <p>It is important to note what the last version created was at https://github.com/singnet/das-metta-parser/tags before creating a new version.</p> <ol> <li>Execute DAS Metta Parser Build Workflow</li> </ol> <p>Initiate the 'DAS Metta Parser Build' Workflow Manually via the 'Actions' Tab in the Repository. Click 'Run workflow' and proceed with the provided instructions, ensuring the master branch is selected. Enter the desired version number in the format 1.0.0, then click 'Run workflow' to proceed.</p> <p></p> <ol> <li>Monitor the workflow execution. Ensure all jobs complete successfully.</li> </ol> <p>Refresh the page and check if a new workflow is running. By clicking on it, you can track all jobs. At the end of the process, all jobs should have a green check mark. If there is an error in any job, it is possible to click on it to view the logs and identify the cause of the problem.</p> <p></p> <ol> <li>Verify the new tag at DAS Metta Parser repository tags and Docker Hub.</li> </ol> <p>It is important to note that this pipeline should generate an image on Docker Hub, following the format 1.0.0-toolbox. Make sure that the image is generated correctly and available at https://hub.docker.com/r/trueagi/das/tags. After the workflow execution, verify if all jobs were successfully completed. The new version tag should be available at https://github.com/singnet/das-metta-parser/tags.</p>"},{"location":"Deployment-Process/#new-version-of-das-toolbox","title":"New version of DAS Toolbox","text":"<ol> <li>Go to the DAS Toolbox repository.</li> </ol> <p>To publish a new version of DAS Toolbox, access the repository at https://github.com/singnet/das-toolbox/.</p> <p></p> <ol> <li>Update the toolbox image version in src/config/config.py</li> </ol> <p>Ensure to update the toolbox image version number in the src/config/config.py file. This is important because syntax check and loader are executed from this toolbox image.</p> <p></p> <ol> <li>Commit this change to the master branch.</li> </ol> <p>After this change, it is necessary to commit to the master branch to record the change.</p> <ol> <li>Verify latest created tag before versioning</li> </ol> <p>It is important to note what the last version created was at https://github.com/singnet/das-toolbox/tags before creating a new version.</p> <ol> <li>Run the \"DAS CLI Build\" workflow from Actions.</li> </ol> <p>Manually execute the \u201cDAS CLI Build\u201d workflow through the \u201cActions\u201d tab in the repository. Click 'Run workflow' and proceed with the provided instructions, ensuring the master branch is selected. Enter the desired version number in the format 1.0.0, then click 'Run workflow' to proceed.</p> <p></p> <ol> <li>Monitor the workflow execution. Ensure all jobs complete successfully.</li> </ol> <p>After the workflow execution, refresh the page and check if a new workflow is running. By clicking on it, you can track all jobs. At the end of the process, all jobs should have a green check mark. If there is an error in any job, it is possible to click on it to view the logs and identify the cause of the problem.</p> <p></p> <ol> <li>Verify the new tag at DAS Toolbox repository tags</li> </ol> <p>After the workflow execution, verify if all jobs were successfully completed. The new version tag should be available at https://github.com/singnet/das-toolbox/tags. Additionally, the CLI file generated by the pipeline will be available for download in the workflow artifacts, allowing its use locally.</p> <p></p>"},{"location":"das-overview/","title":"Distributed Atomspace - Overview","text":"<p>Atomspace is the hypergraph OpenCog Hyperon uses to represent and store knowledge, being the source of knowledge for AI agents and the container of any computational result that might be created or achieved during their execution.</p> <p>The Distributed Atomspace (DAS) is an extension of OpenCog Hyperon's Atomspace into a more independent component designed to support multiple simultaneous connections with different AI algorithms, providing a flexible query interface to distributed knowledge bases. It can be used as a component (e.g. a Python library) or as a stand-alone server to store essentially arbitrarily large knowledge bases and provide means for the agents to traverse regions of the hypergraphs and perform global queries involving properties, connectivity, subgraph topology, etc.</p> <p>DAS can be understood as a persistence layer for knowledge bases used in OpenCog Hyperon.</p> <p></p> <p>The data manipulation API provides a defined set of operations without exposing database details such as data modeling and the DBMS (Database Management System) being used. This is important because it allows us to evolve the data model inside DAS and even change the DBMS without affecting the integration with the AI agents.</p> <p>But being an abstraction for the data model is not the only purpose of DAS. While performing this connection between AI agents and the knowledge bases, DAS provides a lot of other functionalities:</p> <ul> <li>Higher level indexes stored in the DBMS</li> <li>Query engine with pattern matching capabilities</li> <li>Traverse engine to keep track of hypergraph traversal</li> <li>Cache for query results</li> <li>Scalable connection manager to connect the DAS with multiple other DASs</li> </ul> <p>This is why DAS is not just a Data Access Object or a database interface layer but rather a more complex OpenCog Hyperon's component that abstracts not only data modeling/access itself but also several other algorithms that are closely related to the way AI agents manipulate information.</p>"},{"location":"das-overview/#table-of-contents","title":"Table of contents","text":"<ul> <li>DAS Components</li> <li>Higher Level Indexing</li> <li>Pattern Matcher</li> <li>Mapping knowledge bases to nodes and links</li> <li>DAS Server Deployment and Architecture</li> </ul>"},{"location":"das-overview/#das-components","title":"DAS Components","text":"<p>DAS is delivered as a Python library hyperon-das which can be used in two different ways:</p> <ol> <li>To create a DAS server which is supposed to contain a knowledge base and provide it to many remote clients (somehow like a DBMS).</li> <li>To instantiate a DAS in a Python program which can store a smaller local knowledge base and can, optionally, connect to one or more remote DAS servers, exposing their contents to the local program. In this case, the local knowledge base can store its contents in RAM or can use a DB backend to persist it.</li> </ol> <p></p> <p>Components in the DAS architecture are designed to provide the same  data manipulation API regardless of whether it's being used locally or remotely or, in the case of a local DAS, whether DB persistence is being used or not.</p> <p>Part of this API is delegated to Traverse Engine, which interacts with the Query Engine and the Cache to provide means to the user to traverse the Atomspace hypergraph. Operations like finding the links pointing from/to a given atom or finding atoms in the surrounding neighborhood are performed by this engine, which controls the pre-fetching of the surrounding atoms when a remote DAS is being used, in such a way that following links can be done quickly.</p> <p>The Query Engine is where global queries are processed. These are queries for specific atoms or sets of atoms that satisfies some criteria, including pattern matching. When making a query, the user can specify whether only local atoms should be considered or whether atoms in remote DASs should be searched as well. If that's the case, the Query Engine connects to the remote OpenFaaS servers to make the queries in the remote DASs and return a answer which is a proper combination of local and remote information. For instance, if there're different versions of the same atom in local and one of the remote DASs, the local version is returned.</p> <p>Both engines use the Cache in order to make queries involving a remote DAS faster. The DAS' cache is not exactly like a traditional cache, where data is stored basically in the same way in both, the cache and the primary data repository, and queries are answered by searching the data in the former and then in the latter. The DAS's cache implements this functionality but it also sorts and partitions queries' results in such a way that the caller sees the most relevant results first.</p> <p>All the queries that return more than one atom, return an iterator to the results instead of the results themselves. This way only a subset of the results are returned in a remote query. When the caller iterates through this iterator, other chunks of results are fetched on demand from the remote DAS until all the results have been visited. Before splitting the results in chunks, the resulting atoms are sorted by \"relevance\", which can be a measure based in atoms' Short and Long Term Importance (STI and LTI), in a way that the most relevant results are iterated first. This is important because most AI agents make several queries and visit the results in a combinatorial fashion so visiting every single possible combination of results are not practical. Having results sorted by relevance allow the agents to constraint the search and eventually avoid fetching too many chunks of results from the remote server.</p> <p>The AtomDB is somehow like a Data Access Object or a database interface layer to abstract the calls to the database where atoms are actually stored. Having this abstraction is important because it allows us to change or to extend the actual data storage without affecting the query algorithms (such as pattern matching) implemented in traverse and query engines. AtomDB can be backended by in-RAM data structures or one or more DBMSs.</p>"},{"location":"das-overview/#higher-level-indexing","title":"Higher Level Indexing","text":"<p>DAS uses a DBMS to store atoms. By doing so it uses the indexing capabilities of this DBMS to retrieve atoms faster. But in addition to this, DAS also creates other custom indexes and stores these indexes in another DBMS. The most relevant of such indexes is the Pattern Inverted Index.</p> <p>An inverted index is a data structure which stores a map from contents (words, sentences, numbers, etc) to where they can be found in a given data container (database, file system etc).</p> <p>This type of data structure is largely used in document retrieval systems to implement efficient search engines. The idea is spending computational time when documents are inserted in the document base to index and record the words that appear in each document (and possibly the position they happen inside the documents). Afterwards this index can be used by the search engine to efficiently locate documents that contain a given set of keywords.</p> <p>The entities in the Opencog Hyperon's context are different from the ones in typical document retrieval systems but their roles and the general idea of the algorithms are very similar. In OpenCog Hyperon's context, a knowledge base is a set of toplevel links (which may point to nodes or to other links). When the knowledge base is loaded, we can create an inverted index of patterns present in each toplevel link and use such index later to perform pattern matching.</p> <p>For instance, given as toplevel link like this one:</p> <pre><code>Inherits\n  &lt;Concept A&gt;\n  &lt;Concept B&gt;\n</code></pre> <p>We could add entries like these ones in the Pattern Inverted Index (where <code>H1</code> is the handle of the toplevel link above):</p> <pre><code>Inherits * &lt;Concept B&gt; ==&gt; H1\nInherits &lt;Concept A&gt; * ==&gt; H1\nInherits * * ==&gt; H1\n</code></pre>"},{"location":"das-overview/#pattern-matcher","title":"Pattern Matcher","text":"<p>DAS' query engine can answer pattern matching queries. These are queries where the caller specifies a pattern i.e. a boolean expression of subgraphs with nodes, links and wildcards and the engine finds every subgraph in the knowledge base that satisfies the passed expression.</p> <p>For instance, suppose we have the following knowledge base in DAS.</p> <p></p> <p>We could search for a pattern like:</p> <pre><code>AND\n  Similar(V1, V2)\n  NOT\n    AND\n      IS_A(V1, V3)\n      IS_A(V2, V3)\n</code></pre> <p><code>V1</code>, <code>V2</code> and <code>V3</code> are wildcards or variables. In any candidate subgraph answer, the atom replacing <code>V1</code>, for instance, should be the same in all the links where <code>V1</code> appears. In other words, with this pattern we are searching for two nodes <code>V1</code> and <code>V2</code> such that there exist a similarity link between them but there's no pair of inheritance links pointing <code>V1</code> and <code>V2</code> to the same node <code>V3</code>, no matter the value of <code>V3</code>.</p> <p>In this example, <code>Chimp</code> and <code>Human</code> are not a suitable answer to replace <code>V1</code> and <code>V2</code> because there's a possible value for <code>V3</code> that satisfies the <code>AND</code> clause in the pattern, as shown below.</p> <p></p> <p>On the other hand, there are other pair of nodes which could be used to match <code>V1</code> and <code>V2</code> whitout matching the <code>AND</code> clause, as shown below.</p> <p></p> <p>The answer for the query is all the subgraphs that satisfy the pattern. In our example, the answer would be as follows.</p> <p></p>"},{"location":"das-overview/#mapping-knowledge-bases-to-nodes-and-links","title":"Mapping knowledge bases to nodes and links","text":"<p>Before loading a knowledge base into DAS, you need to define a proper mapping to Atomspace nodes and links. DAS doesn't make any assumptions regarding nodes or link types, arity etc. When adding nodes and links using DAS' API, one may specify atom types freely and the semantic meaning of such atom types are totally concerned with the application. DAS don't make any kind of processing based in pre-defined types (actually, there are no internally pre-defined atom types).</p> <p>DAS also doesn't provide a way to read a text or SQL or whatever type of file in order to load a knowledge base. There's no DAS-defined file syntax for this. If one needs to import a knowledge base, it needs to provide a proper loader application to parse the input file(s) and make the proper calls to DAS' API in order to add nodes and links.</p> <p>Surely one of the interesting topics for future/on-going work on DAS is to provide loaders (and respective nodes/links mapping) for different types of knowledge base formats like SQL, Atomese, etc. We already have such a loader for MeTTa files.</p>"},{"location":"das-overview/#das-server-deployment-and-architecture","title":"DAS Server Deployment and Architecture","text":"<p>DAS server is deployed in a Lambda Architecture based either in OpenFaaS or AWS Lambda. We made a comparative study of these two architectures (results are presented in this report) and decided to prioritize OpenFaaS. Although deployment in AWS Lambda is still possible, currently only OpenFaaS is supported by our automated deployment tool. This architecture is presented in the diagram below.</p> <p></p> <p>When deploying in AWS Lambda, Redis and MongoDB can be replaced by AWS' DocumentDB and ElastiCache but the overall structure is basically the same.</p> <p>Functions are deployed in servers in the cloud as Docker containers, built in our CI/CD pipeline by automated GitHub Actions scripts and stored in a private Docker hub registry.</p> <p>Clients can connect using HTTP, gRPC or an external lambda functions (OpenFaaS functions can only connect to OpenFaaS and the same is true for AWS functions).</p> <p>DAS is versioned and released as a library in PyPI.</p>"},{"location":"das-users-guide/","title":"DAS - User's Guide","text":"<p>Atomspace is the hypergraph OpenCog Hyperon uses to represent and store knowledge, being the source of knowledge for AI agents and the container of any computational result that might be created or achieved during their execution.</p> <p>The Distributed Atomspace (DAS) is an extension of OpenCog Hyperon's Atomspace into a more independent component designed to support multiple simultaneous connections with different AI algorithms, providing a flexible query interface to distributed knowledge bases. It can be used as a component (e.g. a Python library) or as a stand-alone server to store essentially arbitrarily large knowledge bases and provide means for the agents to traverse regions of the hypergraphs and perform global queries involving properties, connectivity, subgraph topology, etc.</p> <p>Regardless of being used locally or remotely, DAS provides the exact same API to query or traverse the Atomspace. This API is fully documented here. In this document we provide examples and best practices to use this API with each type of DAS.</p>"},{"location":"das-users-guide/#table-of-contents","title":"Table of contents","text":"<ul> <li>Local DAS with data in RAM<ul> <li>Adding atoms</li> <li>Fetching from a DAS server</li> <li>Getting atoms by their properties</li> <li>Traversing the hypergraph</li> <li>Pattern Matcher Queries</li> </ul> </li> <li>Connecting to a remote DAS<ul> <li>Querying a remote DAS</li> <li>Custom indexes</li> </ul> </li> <li>Starting a DAS Server</li> </ul>"},{"location":"das-users-guide/#local-das-with-data-in-ram","title":"Local DAS with data in RAM","text":"<p>A local DAS stores atoms as Python dicts in RAM. One can create an empty DAS by calling the basic constructor with no parameters.</p> <pre><code>from hyperon_das import DistributedAtomSpace\n\ndas = DistributedAtomSpace()\nprint(das.count_atoms())\n</code></pre> <pre><code>(0, 0)\n</code></pre> <p>This is equivalent to instantiating it passing <code>query_engine='local'</code></p> <pre><code>from hyperon_das import DistributedAtomSpace\n\ndas = DistributedAtomSpace(query_engine='local')\nprint(das.count_atoms())\n</code></pre> <pre><code>(0, 0)\n</code></pre> <p>The <code>query_engine</code> parameter is used to select from <code>local</code> or <code>remote</code> DAS. Remote DAS is explained later in this document.</p> <p>A local DAS can be populated using the methods <code>add_node()</code> and <code>add_link()</code>. Let's use this simple knowledge base as an example.</p> <p> </p> <p>We have only one type of node (e.g. Concept) to represent animals and two types of links, (e.g. Inheritance and Similarity) to represent relations between them.</p> <p></p>"},{"location":"das-users-guide/#adding-atoms","title":"Adding atoms","text":"<p>We can add nodes explicitly by calling <code>add_node()</code> passing a Python dict representing the node. This dict may contain any number of keys associated to values of any type (including lists, sets, nested dicts, etc) , which are all recorded with the node, but must contain at least the keys <code>type</code> and <code>name</code> mapping to strings which define the node uniquely, i.e. two nodes with the same <code>type</code> and <code>name</code> are considered to be the same entity.</p> <p><code>add_link()</code> works mostly in the same way. For links, the mandatory fields are <code>type</code> and <code>targets</code>, which is a list of Python dicts representing either nodes or nested links.</p> <pre><code>    das.add_node({\"type\": \"Concept\", \"name\": \"human\"})\n    das.add_node({\"type\": \"Concept\", \"name\": \"monkey\"})\n    das.add_node({\"type\": \"Concept\", \"name\": \"chimp\"})\n    das.add_node({\"type\": \"Concept\", \"name\": \"mammal\"})\n    das.add_node({\"type\": \"Concept\", \"name\": \"reptile\"})\n    das.add_node({\"type\": \"Concept\", \"name\": \"snake\"})\n    das.add_node({\"type\": \"Concept\", \"name\": \"dinosaur\"})\n    das.add_node({\"type\": \"Concept\", \"name\": \"triceratops\"})\n    das.add_node({\"type\": \"Concept\", \"name\": \"earthworm\"})\n    das.add_node({\"type\": \"Concept\", \"name\": \"rhino\"})\n    das.add_node({\"type\": \"Concept\", \"name\": \"vine\"})\n    das.add_node({\"type\": \"Concept\", \"name\": \"ent\"})\n    das.add_node({\"type\": \"Concept\", \"name\": \"animal\"})\n    das.add_node({\"type\": \"Concept\", \"name\": \"plant\"}) ;\n</code></pre> <p>We can also add nodes implicitly while adding links.</p> <pre><code>das.add_link(\n    {\n        \"type\": \"Similarity\",\n        \"targets\": [\n            {\"type\": \"Concept\", \"name\": \"human\"},\n            {\"type\": \"Concept\", \"name\": \"monkey\"},\n        ],\n    }\n) ;\n</code></pre> <p>\"human\" and \"monkey\" would be inserted if they hadn't been inserted before. Adding the node or link more than once is allowed and has no side effects. So let's add the whole set of links from our knowledge base.</p> <pre><code>das.add_link( { \"type\": \"Similarity\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"human\"}, {\"type\": \"Concept\", \"name\": \"monkey\"}, ], })\ndas.add_link( { \"type\": \"Similarity\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"human\"}, {\"type\": \"Concept\", \"name\": \"chimp\"}, ], })\ndas.add_link( { \"type\": \"Similarity\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"chimp\"}, {\"type\": \"Concept\", \"name\": \"monkey\"}, ], })\ndas.add_link( { \"type\": \"Similarity\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"snake\"}, {\"type\": \"Concept\", \"name\": \"earthworm\"}, ], })\ndas.add_link( { \"type\": \"Similarity\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"rhino\"}, {\"type\": \"Concept\", \"name\": \"triceratops\"}, ], })\ndas.add_link( { \"type\": \"Similarity\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"snake\"}, {\"type\": \"Concept\", \"name\": \"vine\"}, ], })\ndas.add_link( { \"type\": \"Similarity\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"human\"}, {\"type\": \"Concept\", \"name\": \"ent\"}, ], })\ndas.add_link( { \"type\": \"Similarity\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"monkey\"}, {\"type\": \"Concept\", \"name\": \"human\"}, ], })\ndas.add_link( { \"type\": \"Similarity\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"chimp\"}, {\"type\": \"Concept\", \"name\": \"human\"}, ], })\ndas.add_link( { \"type\": \"Similarity\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"monkey\"}, {\"type\": \"Concept\", \"name\": \"chimp\"}, ], })\ndas.add_link( { \"type\": \"Similarity\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"earthworm\"}, {\"type\": \"Concept\", \"name\": \"snake\"}, ], })\ndas.add_link( { \"type\": \"Similarity\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"triceratops\"}, {\"type\": \"Concept\", \"name\": \"rhino\"}, ], })\ndas.add_link( { \"type\": \"Similarity\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"vine\"}, {\"type\": \"Concept\", \"name\": \"snake\"}, ], })\ndas.add_link( { \"type\": \"Similarity\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"ent\"}, {\"type\": \"Concept\", \"name\": \"human\"}, ], })\ndas.add_link( { \"type\": \"Inheritance\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"human\"}, {\"type\": \"Concept\", \"name\": \"mammal\"}, ], })\ndas.add_link( { \"type\": \"Inheritance\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"monkey\"}, {\"type\": \"Concept\", \"name\": \"mammal\"}, ], })\ndas.add_link( { \"type\": \"Inheritance\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"chimp\"}, {\"type\": \"Concept\", \"name\": \"mammal\"}, ], })\ndas.add_link( { \"type\": \"Inheritance\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"mammal\"}, {\"type\": \"Concept\", \"name\": \"animal\"}, ], })\ndas.add_link( { \"type\": \"Inheritance\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"reptile\"}, {\"type\": \"Concept\", \"name\": \"animal\"}, ], })\ndas.add_link( { \"type\": \"Inheritance\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"snake\"}, {\"type\": \"Concept\", \"name\": \"reptile\"}, ], })\ndas.add_link( { \"type\": \"Inheritance\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"dinosaur\"}, {\"type\": \"Concept\", \"name\": \"reptile\"}, ], })\ndas.add_link( { \"type\": \"Inheritance\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"triceratops\"}, {\"type\": \"Concept\", \"name\": \"dinosaur\"}, ], })\ndas.add_link( { \"type\": \"Inheritance\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"earthworm\"}, {\"type\": \"Concept\", \"name\": \"animal\"}, ], })\ndas.add_link( { \"type\": \"Inheritance\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"rhino\"}, {\"type\": \"Concept\", \"name\": \"mammal\"}, ], })\ndas.add_link( { \"type\": \"Inheritance\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"vine\"}, {\"type\": \"Concept\", \"name\": \"plant\"}, ], })\ndas.add_link( { \"type\": \"Inheritance\", \"targets\": [ {\"type\": \"Concept\", \"name\": \"ent\"}, {\"type\": \"Concept\", \"name\": \"plant\"}, ], }) ;\n</code></pre> <p>Links are always asymetric, so symmetric relationships like \"Similarity\" are represented by adding two links. For instance:</p> <pre><code>das.add_link(\n    {\n        \"type\": \"Similarity\",\n        \"targets\": [\n            {\"type\": \"Concept\", \"name\": \"human\"},\n            {\"type\": \"Concept\", \"name\": \"monkey\"},\n        ],\n    }\n)\n</code></pre> <p>and</p> <pre><code>das.add_link(\n    {\n        \"type\": \"Similarity\",\n        \"targets\": [\n            {\"type\": \"Concept\", \"name\": \"monkey\"},\n            {\"type\": \"Concept\", \"name\": \"human\"},\n        ],\n    }\n)\n</code></pre> <p>Considering this, we can print the atom count again.</p> <pre><code>print(das.count_atoms())\n</code></pre> <pre><code>(14, 26)\n</code></pre> <p></p>"},{"location":"das-users-guide/#fetching-from-a-das-server","title":"Fetching from a DAS server","text":"<p>Instead of adding atoms by calling <code>add_node()</code> and <code>add_link()</code> directly, it's possible to fetch all or part of the contents from a DAS server using the method <code>fetch()</code>. This method doesn't create a lasting connection with the DAS server, it will just fetch the atoms once and close the connection so any subsequent changes or queries will not be propagated to the server in any way. After fetching the atoms, all queries will be made locally. It's possible to call <code>fetch()</code> multiple times fetching from the same DAS Server or from different ones.</p> <pre><code>from hyperon_das import DistributedAtomSpace\n\nremote_das_host = \"45.63.85.59\"\nremote_das_port = 8080\n\nimported_das = DistributedAtomSpace()\nprint(imported_das.count_atoms())\n\nlinks_to_import = {\n    'atom_type': 'link',\n    'type': 'Expression',\n    'targets': [\n        {'atom_type': 'node', 'type': 'Symbol', 'name': 'Inheritance'},\n        {'atom_type': 'variable', 'name': 'v2'},\n        {'atom_type': 'variable', 'name': 'v3'},\n    ]\n}\n\nimported_das.fetch(links_to_import, remote_das_host, remote_das_port)\nprint(imported_das.count_atoms())\n</code></pre> <pre><code>(0, 0)\n(15, 12)\n</code></pre> <p>The first parameter of <code>fetch()</code> is a pattern to describe which atoms should be fetched. It's exactly the same pattern used to make pattern matching.</p> <p></p>"},{"location":"das-users-guide/#getting-atoms-by-their-properties","title":"Getting atoms by their properties","text":"<p>DAS has an API to query atoms by their properties. Most of this API is based on atom handles. Handles are MD5 signatures associated with atoms. For now they are supposed to be unique ids for atoms although this is not 100% true (conflict handling is planned to be implemented in the near future). DAS provides two static methods to compute handles for nodes and links: <code>das.get_node_handle()</code> and <code>das.get_link_handle()</code>.</p> <pre><code>human = das.get_node_handle('Concept', 'human')\nent = das.get_node_handle('Concept', 'ent')\n\nprint(\"human:\", human)\nprint(\"ent:\", ent)\n\nsimilarity_link = das.get_link_handle('Similarity', [human, ent])\n\nprint(\"Similarity link:\", similarity_link)\n</code></pre> <pre><code>human: af12f10f9ae2002a1607ba0b47ba8407\nent: 4e8e26e3276af8a5c2ac2cc2dc95c6d2\nSimilarity link: 16f7e407087bfa0b35b13d13a1aadcae\n</code></pre> <p>Note that these are static methods which don't actually query the stored atomspace in order to compute those handles. Instead, they just run a MD5 hashing algorithm over the data that uniquely identifies nodes and links, i.e. node type and name in the case of nodes and link type and targets in the case of links. This means e.g. that two nodes with the same type and the same name are considered to be the exact same entity.</p> <p>Atom handles can be used to retrieve the actual atom document.</p> <pre><code>das.get_atom(human)\n</code></pre> <pre><code>{'handle': 'af12f10f9ae2002a1607ba0b47ba8407',\n 'type': 'Concept',\n 'composite_type_hash': 'd99a604c79ce3c2e76a2f43488d5d4c3',\n 'name': 'human',\n 'named_type': 'Concept'}\n</code></pre> <p>Convenience methods can be used to retrieve atoms passing its basic properties instead.</p> <pre><code>print(\"human:\", das.get_node('Concept', 'human'))\nprint(\"\\nSimilarity link:\", das.get_link('Similarity', [human, ent]))\n</code></pre> <pre><code>human: {'handle': 'af12f10f9ae2002a1607ba0b47ba8407', 'type': 'Concept', 'composite_type_hash': 'd99a604c79ce3c2e76a2f43488d5d4c3', 'name': 'human', 'named_type': 'Concept'}\n\nSimilarity link: {'handle': '16f7e407087bfa0b35b13d13a1aadcae', 'type': 'Similarity', 'composite_type_hash': 'ed73ea081d170e1d89fc950820ce1cee', 'is_toplevel': True, 'composite_type': ['a9dea78180588431ec64d6bc4872fdbc', 'd99a604c79ce3c2e76a2f43488d5d4c3', 'd99a604c79ce3c2e76a2f43488d5d4c3'], 'named_type': 'Similarity', 'named_type_hash': 'a9dea78180588431ec64d6bc4872fdbc', 'targets': ['af12f10f9ae2002a1607ba0b47ba8407', '4e8e26e3276af8a5c2ac2cc2dc95c6d2']}\n</code></pre> <p>It's possible to get all links pointing to a specific atom.</p> <pre><code># All links pointing from/to 'rhino'\n\nrhino = das.get_node_handle('Concept', 'rhino')\nlinks = das.get_incoming_links(rhino)\nfor link in links:\n    print(link['type'], link['targets'])\n</code></pre> <pre><code>Similarity ['d03e59654221c1e8fcda404fd5c8d6cb', '99d18c702e813b07260baf577c60c455']\nSimilarity ['99d18c702e813b07260baf577c60c455', 'd03e59654221c1e8fcda404fd5c8d6cb']\nInheritance ['99d18c702e813b07260baf577c60c455', 'bdfe4e7a431f73386f37c6448afe5840']\n</code></pre> <p>Links can also be retrieved by other properties or partial definition of its main properties (type and targets). The method <code>get_links()</code> can be used passing different combinations of parameters.</p> <pre><code># All inheritance links\n\nlinks = das.get_links(link_type='Inheritance')\nfor link in links:\n    print(link['type'], link['targets'])                      \n</code></pre> <pre><code>Inheritance ['5b34c54bee150c04f9fa584b899dc030', 'bdfe4e7a431f73386f37c6448afe5840']\nInheritance ['b94941d8cd1c0ee4ad3dd3dcab52b964', '80aff30094874e75028033a38ce677bb']\nInheritance ['bb34ce95f161a6b37ff54b3d4c817857', '0a32b476852eeb954979b87f5f6cb7af']\nInheritance ['c1db9b517073e51eb7ef6fed608ec204', 'b99ae727c787f1b13b452fd4c9ce1b9a']\nInheritance ['bdfe4e7a431f73386f37c6448afe5840', '0a32b476852eeb954979b87f5f6cb7af']\nInheritance ['1cdffc6b0b89ff41d68bec237481d1e1', 'bdfe4e7a431f73386f37c6448afe5840']\nInheritance ['af12f10f9ae2002a1607ba0b47ba8407', 'bdfe4e7a431f73386f37c6448afe5840']\nInheritance ['b99ae727c787f1b13b452fd4c9ce1b9a', '0a32b476852eeb954979b87f5f6cb7af']\nInheritance ['4e8e26e3276af8a5c2ac2cc2dc95c6d2', '80aff30094874e75028033a38ce677bb']\nInheritance ['d03e59654221c1e8fcda404fd5c8d6cb', '08126b066d32ee37743e255a2558cccd']\nInheritance ['99d18c702e813b07260baf577c60c455', 'bdfe4e7a431f73386f37c6448afe5840']\nInheritance ['08126b066d32ee37743e255a2558cccd', 'b99ae727c787f1b13b452fd4c9ce1b9a']\n</code></pre> <pre><code># Inheritance links between two Concept nodes\n\nlinks = das.get_links(link_type='Inheritance', target_types=['Concept', 'Concept'])\nfor link in links:\n    print(link['type'], link['targets'])   \n</code></pre> <pre><code>Inheritance ['5b34c54bee150c04f9fa584b899dc030', 'bdfe4e7a431f73386f37c6448afe5840']\nInheritance ['b94941d8cd1c0ee4ad3dd3dcab52b964', '80aff30094874e75028033a38ce677bb']\nInheritance ['bb34ce95f161a6b37ff54b3d4c817857', '0a32b476852eeb954979b87f5f6cb7af']\nInheritance ['c1db9b517073e51eb7ef6fed608ec204', 'b99ae727c787f1b13b452fd4c9ce1b9a']\nInheritance ['bdfe4e7a431f73386f37c6448afe5840', '0a32b476852eeb954979b87f5f6cb7af']\nInheritance ['1cdffc6b0b89ff41d68bec237481d1e1', 'bdfe4e7a431f73386f37c6448afe5840']\nInheritance ['af12f10f9ae2002a1607ba0b47ba8407', 'bdfe4e7a431f73386f37c6448afe5840']\nInheritance ['b99ae727c787f1b13b452fd4c9ce1b9a', '0a32b476852eeb954979b87f5f6cb7af']\nInheritance ['4e8e26e3276af8a5c2ac2cc2dc95c6d2', '80aff30094874e75028033a38ce677bb']\nInheritance ['d03e59654221c1e8fcda404fd5c8d6cb', '08126b066d32ee37743e255a2558cccd']\nInheritance ['99d18c702e813b07260baf577c60c455', 'bdfe4e7a431f73386f37c6448afe5840']\nInheritance ['08126b066d32ee37743e255a2558cccd', 'b99ae727c787f1b13b452fd4c9ce1b9a']\n</code></pre> <pre><code># Similarity links where 'snake' is the first target\n\nsnake = das.get_node_handle('Concept', 'snake')\nlinks = das.get_links(link_type='Similarity', link_targets=[snake, '*'])\nfor link in links:\n    print(link['type'], link['targets']) \n</code></pre> <pre><code>Similarity ['c1db9b517073e51eb7ef6fed608ec204', 'b94941d8cd1c0ee4ad3dd3dcab52b964']\nSimilarity ['c1db9b517073e51eb7ef6fed608ec204', 'bb34ce95f161a6b37ff54b3d4c817857']\n</code></pre> <pre><code># Any links where 'snake' is the first target\n\nsnake = das.get_node_handle('Concept', 'snake')\nlinks = das.get_links(link_type='*', link_targets=[snake, '*'])\nfor link in links:\n    print(link['type'], link['targets']) \n</code></pre> <pre><code>Similarity ['c1db9b517073e51eb7ef6fed608ec204', 'b94941d8cd1c0ee4ad3dd3dcab52b964']\nInheritance ['c1db9b517073e51eb7ef6fed608ec204', 'b99ae727c787f1b13b452fd4c9ce1b9a']\nSimilarity ['c1db9b517073e51eb7ef6fed608ec204', 'bb34ce95f161a6b37ff54b3d4c817857']\n</code></pre> <p></p>"},{"location":"das-users-guide/#traversing-the-hypergraph","title":"Traversing the hypergraph","text":"<p>It's possible to traverse the hypergraph using a <code>TraverseEngine</code> which is like a cursor that can be moved through nodes and links. First, let's initiate a <code>TraverseEngine</code> pointing to \"human\". In order to do this, we need to call <code>get_traversal_cursor()</code> passing the handle of the atom to be used as the starting point for the traversing. This atom can be either a link or a node. We'll use the method <code>das.get_node_handle()</code> to get the handle of the Concept \"human\" and start on it.</p> <pre><code>cursor = das.get_traversal_cursor(das.get_node_handle('Concept', 'human'))\n</code></pre> <p>Once we have a cursor we can get the whole document of the atom pointed by it:</p> <pre><code>cursor.get()\n</code></pre> <pre><code>{'handle': 'af12f10f9ae2002a1607ba0b47ba8407',\n 'type': 'Concept',\n 'composite_type_hash': 'd99a604c79ce3c2e76a2f43488d5d4c3',\n 'name': 'human',\n 'named_type': 'Concept'}\n</code></pre> <p>We can also see all links that make reference to cursor. Optional parameters can be used to filter which links should be considered. Here are some examples. We're printing only link type and targets to make the output cleaner.</p> <pre><code># All links pointing from/to cursor\nprint(\"All links:\", [(d['type'], d['targets']) for d in cursor.get_links()])\n\n# Only Inheritance links\nprint(\"\\nInheritance links:\", [(d['type'], d['targets']) for d in cursor.get_links(link_type='Inheritance')])\n\n# Links whose first target is our cursor\nprint(\"\\n'human' is first link target:\", [(d['type'], d['targets']) for d in cursor.get_links(cursor_position=0)])\n</code></pre> <pre><code>All links: [('Similarity', ['af12f10f9ae2002a1607ba0b47ba8407', '4e8e26e3276af8a5c2ac2cc2dc95c6d2']), ('Inheritance', ['af12f10f9ae2002a1607ba0b47ba8407', 'bdfe4e7a431f73386f37c6448afe5840']), ('Similarity', ['af12f10f9ae2002a1607ba0b47ba8407', '5b34c54bee150c04f9fa584b899dc030']), ('Similarity', ['1cdffc6b0b89ff41d68bec237481d1e1', 'af12f10f9ae2002a1607ba0b47ba8407']), ('Similarity', ['4e8e26e3276af8a5c2ac2cc2dc95c6d2', 'af12f10f9ae2002a1607ba0b47ba8407']), ('Similarity', ['af12f10f9ae2002a1607ba0b47ba8407', '1cdffc6b0b89ff41d68bec237481d1e1']), ('Similarity', ['5b34c54bee150c04f9fa584b899dc030', 'af12f10f9ae2002a1607ba0b47ba8407'])]\n\nInheritance links: [('Inheritance', ['af12f10f9ae2002a1607ba0b47ba8407', 'bdfe4e7a431f73386f37c6448afe5840'])]\n\n'human' is first link target: [('Similarity', ['af12f10f9ae2002a1607ba0b47ba8407', '4e8e26e3276af8a5c2ac2cc2dc95c6d2']), ('Inheritance', ['af12f10f9ae2002a1607ba0b47ba8407', 'bdfe4e7a431f73386f37c6448afe5840']), ('Similarity', ['af12f10f9ae2002a1607ba0b47ba8407', '5b34c54bee150c04f9fa584b899dc030']), ('Similarity', ['af12f10f9ae2002a1607ba0b47ba8407', '1cdffc6b0b89ff41d68bec237481d1e1'])]\n</code></pre> <p>There are other possibilities for filtering such as custom filter methods, target types, etc. They're explained in the DAS API.</p> <p>There are also convenience methods to get the cursor's \"neighbors\", which are the other atoms pointed by the links attached to the cursor. Let's investigate the neighbors of \"human\". Again, we can use the same filters to select which links and targets to consider in order to get the neighbors of the cursor.</p> <pre><code># All \"human\" neighbors\nprint(\"All neighbors:\", [(d['type'], d['name']) for d in cursor.get_neighbors()])\n\n# Only neighbors linked through Inheritance links\nprint(\"\\nInheritance relations:\", [(d['type'], d['name']) for d in cursor.get_neighbors(link_type='Inheritance')])\n\n# Only neighbors that are similar to \"human\" (i.e. they share a Similarity link)\nprint(\"\\nSimilar to 'human':\", [(d['type'], d['name']) for d in cursor.get_neighbors(link_type='Similarity', cursor_position=0)])\n</code></pre> <pre><code>All neighbors: [('Concept', 'ent'), ('Concept', 'mammal'), ('Concept', 'chimp'), ('Concept', 'monkey')]\n\nInheritance relations: [('Concept', 'mammal')]\n\nSimilar to 'human': [('Concept', 'ent'), ('Concept', 'chimp'), ('Concept', 'monkey')]\n</code></pre> <p> <code>get_links()</code> and <code>get_neighbors()</code> use the DAS Cache system to sort the atoms before they are returned to the caller. In addition to this, these methods return an iterator rather than an actual list of atoms and this iterator is controlled by the cache system as well. The idea here is that atoms may have a large number of links (and consequently neighbors) attached to it so the AI/ML agent may not be interested in iterating on all of them. Atoms are presented in such a way that high importance atoms tend to be presented first while low importance atoms tend to be presented later.</p> <p>We can move the cursor by following its links. </p> <pre><code>cursor = das.get_traversal_cursor(das.get_node_handle('Concept', 'human'))\ncursor.follow_link()\ncursor.get()\n</code></pre> <pre><code>{'handle': '4e8e26e3276af8a5c2ac2cc2dc95c6d2',\n 'type': 'Concept',\n 'composite_type_hash': 'd99a604c79ce3c2e76a2f43488d5d4c3',\n 'name': 'ent',\n 'named_type': 'Concept'}\n</code></pre> <p><code>follow_link()</code> just gets the first link returned by <code>get_links()</code> in order to follow it and select a target. The same filters described above can be used here to constraint the links/targets that will be considered. For instance we could use the following code to get the most abstract concept (considering our Inheritance links) starting from \"human\".</p> <pre><code>cursor = das.get_traversal_cursor(das.get_node_handle('Concept', 'human'))\nbase = cursor.get()['name']\nwhile True:\n    print(base)\n    cursor.follow_link(link_type='Inheritance', cursor_position=0)\n    if cursor.get()['name'] == base:\n        break\n    base = cursor.get()['name']\ncursor.get()\n</code></pre> <pre><code>human\nmammal\nanimal\n\n\n\n\n\n{'handle': '0a32b476852eeb954979b87f5f6cb7af',\n 'type': 'Concept',\n 'composite_type_hash': 'd99a604c79ce3c2e76a2f43488d5d4c3',\n 'name': 'animal',\n 'named_type': 'Concept'}\n</code></pre> <p></p>"},{"location":"das-users-guide/#pattern-matcher-queries","title":"Pattern Matcher Queries","text":"<p>DAS can answer pattern matching queries. These are queries where the caller specifies a pattern i.e. a boolean expression of subgraphs with nodes, links and wildcards and the engine finds every subgraph in the knowledge base that satisfies the passed expression. Patterns are a list of Python dicts describing a subgraph with wildcards.</p> <p>The method <code>query()</code> expects a pattern and outputs a list of <code>QueryAnswer</code>. Each element in such a list has the variable assignment that satisfies the pattern and the subgraph which is the pattern itself rewritten using the given assignment.</p> <pre><code># This is a pattern like:\n#\n# Inheritance\n#     v1\n#     plant\n#\n# The expected answer is all Inheritance links whose second target == 'plant'\n#\nquery = {\n    'atom_type': 'link',\n    'type': 'Inheritance',\n    'targets': [\n        {'atom_type': 'variable', 'name': 'v1'},\n        {'atom_type': 'node', 'type': 'Concept', 'name': 'plant'},\n    ]\n}\n\nfor query_answer in das.query(query):\n    print(query_answer.assignment)\n    atom_matching_v1 = das.get_atom(query_answer.assignment.mapping['v1'])\n    print(\"v1:\", atom_matching_v1['type'], atom_matching_v1['name'])\n    rewrited_query = query_answer.subgraph\n    print(rewrited_query)\n    print()\n</code></pre> <pre><code>[('v1', 'b94941d8cd1c0ee4ad3dd3dcab52b964')]\nv1: Concept vine\n{'handle': 'e4685d56969398253b6f77efd21dc347', 'type': 'Inheritance', 'composite_type_hash': '41c082428b28d7e9ea96160f7fd614ad', 'is_toplevel': True, 'composite_type': ['e40489cd1e7102e35469c937e05c8bba', 'd99a604c79ce3c2e76a2f43488d5d4c3', 'd99a604c79ce3c2e76a2f43488d5d4c3'], 'named_type': 'Inheritance', 'named_type_hash': 'e40489cd1e7102e35469c937e05c8bba', 'targets': [{'handle': 'b94941d8cd1c0ee4ad3dd3dcab52b964', 'type': 'Concept', 'composite_type_hash': 'd99a604c79ce3c2e76a2f43488d5d4c3', 'name': 'vine', 'named_type': 'Concept'}, {'handle': '80aff30094874e75028033a38ce677bb', 'type': 'Concept', 'composite_type_hash': 'd99a604c79ce3c2e76a2f43488d5d4c3', 'name': 'plant', 'named_type': 'Concept'}]}\n\n[('v1', '4e8e26e3276af8a5c2ac2cc2dc95c6d2')]\nv1: Concept ent\n{'handle': 'ee1c03e6d1f104ccd811cfbba018451a', 'type': 'Inheritance', 'composite_type_hash': '41c082428b28d7e9ea96160f7fd614ad', 'is_toplevel': True, 'composite_type': ['e40489cd1e7102e35469c937e05c8bba', 'd99a604c79ce3c2e76a2f43488d5d4c3', 'd99a604c79ce3c2e76a2f43488d5d4c3'], 'named_type': 'Inheritance', 'named_type_hash': 'e40489cd1e7102e35469c937e05c8bba', 'targets': [{'handle': '4e8e26e3276af8a5c2ac2cc2dc95c6d2', 'type': 'Concept', 'composite_type_hash': 'd99a604c79ce3c2e76a2f43488d5d4c3', 'name': 'ent', 'named_type': 'Concept'}, {'handle': '80aff30094874e75028033a38ce677bb', 'type': 'Concept', 'composite_type_hash': 'd99a604c79ce3c2e76a2f43488d5d4c3', 'name': 'plant', 'named_type': 'Concept'}]}\n</code></pre> <pre><code># This is a pattern like:\n#\n# AND\n#     Inheritance\n#         v1\n#         mammal\n#     Inheritance\n#         v2\n#         dinosaur\n#     Similarity\n#         v1\n#         v2\n#\n# The expected answer is all pair of animals such that \n# one inherits from mammal, the other inherits from dinosaur \n# and they have a Similarity link between them.\n#\nexp1 = {\n    'atom_type': 'link',\n    'type': 'Inheritance',\n    'targets': [\n        {'atom_type': 'variable', 'name': 'v1'},\n        {'atom_type': 'node', 'type': 'Concept', 'name': 'mammal'},\n    ]\n}\nexp2 = {\n    'atom_type': 'link',\n    'type': 'Inheritance',\n    'targets': [\n        {'atom_type': 'variable', 'name': 'v2'},\n        {'atom_type': 'node', 'type': 'Concept', 'name': 'dinosaur'},\n    ]\n}\nexp3 = {\n    'atom_type': 'link',\n    'type': 'Similarity',\n    'targets': [\n        {'atom_type': 'variable', 'name': 'v1'},\n        {'atom_type': 'variable', 'name': 'v2'},\n    ]\n}\nquery = [exp1, exp2, exp3] # a list of expressions mean an AND of them\n\nfor query_answer in das.query(query):\n    print(query_answer.assignment)\n    atom_matching_v1 = das.get_atom(query_answer.assignment.mapping['v1'])\n    atom_matching_v2 = das.get_atom(query_answer.assignment.mapping['v2'])\n    print(\"v1:\", atom_matching_v1['type'], atom_matching_v1['name'])\n    print(\"v2:\", atom_matching_v2['type'], atom_matching_v2['name'])\n    #rewrited_query = query_answer.subgraph\n    #print(rewrited_query)\n    print()\n</code></pre> <pre><code>[('v1', '99d18c702e813b07260baf577c60c455'), ('v2', 'd03e59654221c1e8fcda404fd5c8d6cb')]\nv1: Concept rhino\nv2: Concept triceratops\n</code></pre> <pre><code># This is a pattern like:\n#\n# AND\n#     Similarity\n#         v1\n#         v2\n#     Similarity\n#         v2\n#         v3\n#     Similarity\n#         v3\n#         v1\n#\n# The expected answer is all triplet of animals such that \n# all of them have a Similarity link with the other two.\n#\nexp1 = {\n    'atom_type': 'link',\n    'type': 'Similarity',\n    'targets': [\n        {'atom_type': 'variable', 'name': 'v1'},\n        {'atom_type': 'variable', 'name': 'v2'},\n    ]\n}\nexp2 = {\n    'atom_type': 'link',\n    'type': 'Similarity',\n    'targets': [\n        {'atom_type': 'variable', 'name': 'v2'},\n        {'atom_type': 'variable', 'name': 'v3'},\n    ]\n}\nexp3 = {\n    'atom_type': 'link',\n    'type': 'Similarity',\n    'targets': [\n        {'atom_type': 'variable', 'name': 'v3'},\n        {'atom_type': 'variable', 'name': 'v1'},\n    ]\n}\nquery = [exp1, exp2, exp3] # a list of expressions mean an AND of them\n\nfor query_answer in das.query(query):\n    atom_matching_v1 = das.get_atom(query_answer.assignment.mapping['v1'])\n    atom_matching_v2 = das.get_atom(query_answer.assignment.mapping['v2'])\n    atom_matching_v3 = das.get_atom(query_answer.assignment.mapping['v3'])\n    print(\"v1:\", atom_matching_v1['type'], atom_matching_v1['name'])\n    print(\"v2:\", atom_matching_v2['type'], atom_matching_v2['name'])\n    print(\"v3:\", atom_matching_v3['type'], atom_matching_v3['name'])\n    print()\n</code></pre> <pre><code>v1: Concept monkey\nv2: Concept chimp\nv3: Concept human\n\nv1: Concept human\nv2: Concept monkey\nv3: Concept chimp\n\nv1: Concept chimp\nv2: Concept monkey\nv3: Concept human\n\nv1: Concept monkey\nv2: Concept human\nv3: Concept chimp\n\nv1: Concept human\nv2: Concept chimp\nv3: Concept monkey\n\nv1: Concept chimp\nv2: Concept human\nv3: Concept monkey\n</code></pre> <p></p>"},{"location":"das-users-guide/#connecting-to-a-remote-das","title":"Connecting to a remote DAS","text":"<p>When a DAS is instantiated with a remote query engine, it will connect to a DAS Server previously populated with a knowledge base. Atoms in the remote DAS Server become available for fetching, querying and modification.</p> <p>In addition to the remote DAS, an internal local DAS is also kept locally. Some of the methods in the API will look for atoms first in this local DAS before going to the remote one. Other methods can be configured to search only in one of them (remote or local) or in both. We'll explain this behavior on a case by case basis.</p> <p>In our example, we'll connect to a DAS Server pre-loaded with the following MeTTa expressions:</p> <pre><code>(: Similarity Type)\n(: Concept Type)\n(: Inheritance Type)\n(: \"human\" Concept)\n(: \"monkey\" Concept)\n(: \"chimp\" Concept)\n(: \"snake\" Concept)\n(: \"earthworm\" Concept)\n(: \"rhino\" Concept)\n(: \"triceratops\" Concept)\n(: \"vine\" Concept)\n(: \"ent\" Concept)\n(: \"mammal\" Concept)\n(: \"animal\" Concept)\n(: \"reptile\" Concept)\n(: \"dinosaur\" Concept)\n(: \"plant\" Concept)\n(Similarity \"human\" \"monkey\")\n(Similarity \"human\" \"chimp\")\n(Similarity \"chimp\" \"monkey\")\n(Similarity \"snake\" \"earthworm\")\n(Similarity \"rhino\" \"triceratops\")\n(Similarity \"snake\" \"vine\")\n(Similarity \"human\" \"ent\")\n(Inheritance \"human\" \"mammal\")\n(Inheritance \"monkey\" \"mammal\")\n(Inheritance \"chimp\" \"mammal\")\n(Inheritance \"mammal\" \"animal\")\n(Inheritance \"reptile\" \"animal\")\n(Inheritance \"snake\" \"reptile\")\n(Inheritance \"dinosaur\" \"reptile\")\n(Inheritance \"triceratops\" \"dinosaur\")\n(Inheritance \"earthworm\" \"animal\")\n(Inheritance \"rhino\" \"mammal\")\n(Inheritance \"vine\" \"plant\")\n(Inheritance \"ent\" \"plant\")\n(Similarity \"monkey\" \"human\")\n(Similarity \"chimp\" \"human\")\n(Similarity \"monkey\" \"chimp\")\n(Similarity \"earthworm\" \"snake\")\n(Similarity \"triceratops\" \"rhino\")\n(Similarity \"vine\" \"snake\")\n(Similarity \"ent\" \"human\")\n</code></pre> <p>Semantically, this is the same knowledge base we used as an example for a local DAS above. However, the mapping to nodes and links is slightly different as described in the DAS MeTTa Parser documentation. For instance, each expression, like:</p> <pre><code>(Similarity \"ent\" \"human\")\n</code></pre> <p>is mapped to 4 atoms. 3 nodes and 1 link as follows.</p> <pre><code>{\n    'type': 'Expression',\n    'targets': [\n        {'type': 'Symbol', 'name', 'Similarity'},\n        {'type': 'Symbol', 'name', '\"ent\"'},\n        {'type': 'Symbol', 'name', '\"human\"'}\n    ]\n}\n</code></pre> <pre><code>from hyperon_das import DistributedAtomSpace\n\nhost = '45.63.85.59'\nport = '8080'\n\nremote_das = DistributedAtomSpace(query_engine='remote', host=host, port=port)\nprint(f\"Connected to DAS Server at {host}:{port}\")\n\nprint(\"(nodes, links) =\", remote_das.count_atoms())\n\n</code></pre> <pre><code>Connected to DAS Server at 45.63.85.59:8080\n(nodes, links) = (23, 60)\n</code></pre> <p>Atoms can be retrieved by their properties using <code>get_atom()</code>, <code>get_node()</code>, <code>get_link()</code>, <code>get_incoming_links()</code> and <code>get_links()</code> in the same way described here for local DAS. The only difference is that the local DAS will be searched first for <code>get_atom()</code>, <code>get_node()</code>, <code>get_link()</code> before going to the remote DAS when the atom is not found locally. <code>get_incoming_links()</code> and <code>get_links()</code> will search in both, local and remote DAS, and return an iterator to the results. As we explain here, these iterators use the cache system to sort the results and determine how atoms are fetched from the remote DAS.</p> <p><code>add_node()</code> and <code>add_link()</code> will add atoms only in the local DAS. If you add an atom that already exists in the remote DAS, the local copy is always returned by the methods above. To propagate changes to the remote DAS one needs to call <code>commit()</code>. We'll not provide examples of changes in the remote DAS here because we're using a single DAS Server to serve tests with this animals KB so if you commit changes to it everyone will be affected. So please don't use this notebook to commit changes to our test server.</p> <p><code>fetch()</code> also works in the same way (described here) for a remote DAS. The only difference is that now the caller can omit the parameters for <code>host</code> and <code>port</code> which are defaulted to the connected remote DAS Server. Fetching from a different DAS Server is still possible by setting the proper values for <code>host</code> and <code>port</code>.</p> <p>If you execute the cells below you'll notice a delay between each call. This is because the cache system is not in place yet so each call is issuing an actual query to the remote DAS.</p> <pre><code># Compute the handle and get the actual document for \"symbol\"\nsymbol = '\"earthworm\"'\nsymbol_handle = remote_das.get_node_handle('Symbol', symbol)\nsymbol_document = remote_das.get_atom(symbol_handle)\nsymbol_document\n</code></pre> <pre><code>{'handle': '665509d366ac3c2821b3b6b266f996bd',\n 'type': 'Symbol',\n 'composite_type_hash': '02c86eb2792f3262c21d030a87e19793',\n 'name': '\"earthworm\"',\n 'named_type': 'Symbol',\n 'is_literal': True}\n</code></pre> <pre><code># Get expressions like (* base_symbol *)\niterator = remote_das.get_links(link_type='Expression', link_targets=['*', symbol_handle, '*'])\nfor link in iterator:\n    atom1 = remote_das.get_atom(link['targets'][0])\n    atom2 = remote_das.get_atom(link['targets'][2])\n    print(f\"({atom1['name']} {symbol} {atom2['name']})\")\n</code></pre> <pre><code>(: \"earthworm\" Concept)\n(Inheritance \"earthworm\" \"animal\")\n(Similarity \"earthworm\" \"snake\")\n</code></pre> <pre><code># Re-adding an existing atom with a custom field\nremote_das.add_node(\n    {\n        'type': 'Symbol',\n        'name': symbol,\n        'truth_value': tuple([0.1, 0.9])\n    }\n)\nremote_das.get_node('Symbol', symbol)\n</code></pre> <pre><code>{'handle': '665509d366ac3c2821b3b6b266f996bd',\n 'type': 'Symbol',\n 'composite_type_hash': '02c86eb2792f3262c21d030a87e19793',\n 'name': '\"earthworm\"',\n 'named_type': 'Symbol',\n 'truth_value': (0.1, 0.9)}\n</code></pre> <pre><code># Add (to the local DAS only) a new expression mentioning the base_symbol\nremote_das.add_link(\n    { \n        'type': 'Expression', \n        'targets': [ \n            {'type': 'Symbol', 'name': 'Pos'}, \n            {'type': 'Symbol', 'name': symbol},\n            {'type': 'Symbol', 'name': 'noun'}\n        ]\n    }\n)\n# Get expressions like (* base_symbol *) again\niterator = remote_das.get_links(link_type='Expression', link_targets=['*', symbol_handle, '*'])\nfor link in iterator:\n    atom1 = remote_das.get_atom(link['targets'][0])\n    atom2 = remote_das.get_atom(link['targets'][2])\n    print(f\"({atom1['name']} {symbol} {atom2['name']})\")\n</code></pre> <pre><code>(Pos \"earthworm\" noun)\n(: \"earthworm\" Concept)\n(Inheritance \"earthworm\" \"animal\")\n(Similarity \"earthworm\" \"snake\")\n</code></pre> <p>The methods for traversing the hypergraph work basically in the same way as for the local DAS (this is described here). Because of the way MeTTa expressions are mapped to nodes/links with only one type of node and one type of link, traversing is less intuitive from a human perspective but it still makes sense to implement algorithms. Local and remote DAS are considered by the <code>TraverseEngine</code> and the whole logic of this component is subject to the cache management rules, i.e., the cache will try to pre-fetch atoms and present query answers prioritizing more relevant atoms as the caller navigates through the atomspace hypergraph.</p> <p></p>"},{"location":"das-users-guide/#querying-a-remote-das","title":"Querying a remote DAS","text":"<p>The Pattern Matcher in a remote DAS works basically in the same way as in a local DAS (this is described here). The main difference is the optional parameter <code>query_scope</code> which can be used to define the scope of the query as <code>local_only</code>, <code>remote_only</code> or <code>local_and_remote</code> (its default value is <code>remote_only</code>).</p> <pre><code>query = {\n    'atom_type': 'link',\n    'type': 'Expression',\n    'targets': [\n        {'atom_type': 'variable', 'name': 'v1'},\n        {'atom_type': 'node', 'type': 'Symbol', 'name': symbol},\n        {'atom_type': 'variable', 'name': 'v2'}\n    ]\n}\n\n# The default is to query remote_only\nresults = remote_das.query(query)\nprint(\"Remote only\")\nfor query_answer in results:\n    v1_atom = query_answer[1]['targets'][0]\n    v2_atom = query_answer[1]['targets'][2]\n    print(f\"({v1_atom['name']} {symbol} {v2_atom['name']})\")\n\nresults = remote_das.query(query, {'query_scope': 'local_only'})\nprint()\nprint(\"Local only\")\nfor query_answer in results:\n    v1_atom = query_answer.subgraph['targets'][0]\n    v2_atom = query_answer.subgraph['targets'][2]\n    print(f\"({v1_atom['name']} {symbol} {v2_atom['name']})\")\n\n# local_and_remote is not implemented yet\n#results = remote_das.query(query, {'query_scope': 'local_and_remote'})\n#print(\"Remote + Local\")\n#for query_answer in results:\n#    v1_atom = query_answer[1]['targets'][0]\n#    v2_atom = query_answer[1]['targets'][2]\n#    print(f\"({v1_atom['name']} {symbol} {v2_atom['name']})\")\n\n</code></pre> <pre><code>Remote only\n(Inheritance \"earthworm\" \"animal\")\n(: \"earthworm\" Concept)\n(Similarity \"earthworm\" \"snake\")\n\nLocal only\n(Pos \"earthworm\" noun)\n</code></pre> <p></p>"},{"location":"das-users-guide/#custom-indexes","title":"Custom Indexes","text":"<p>Remote DAS allow creation of custom indexes based on custom fields in nodes or links. These indexes can be used to make subsequent custom queries.</p> <pre><code>symbol_name_index = remote_das.create_field_index('node', 'name', type='Symbol')\nresults = remote_das.custom_query(symbol_name_index, name='\"human\"')\nfor atom in results:\n    print(atom['type'], atom['name'])\n</code></pre> <pre><code>Symbol \"human\"\n</code></pre> <p>In this example, we're creating an index for the field <code>name</code> in nodes. <code>name</code> is supposed to be defined in every node of the knowledge base. To create an index on a field which is defined only for a certain type of node, an extra <code>type</code> parameter should be passed to define which type of nodes should enter in the index: e.g. <code>remote_das.create_field_index('node', 'lemma', type='Word')</code> would create an index for the field <code>lemma</code> on all nodes of type <code>Word</code>. This type of index works only for string or number (integer or floating point) fields. Indexes for links can be created likewise.</p> <p></p>"},{"location":"das-users-guide/#starting-a-das-server","title":"Starting a DAS Server","text":"<p>A DAS Server can be set up using the DAS Toolbox following these steps:</p> <ol> <li>Setup environment variables</li> <li>Start DB servers</li> <li>Load MeTTa knowledge base</li> <li>Start FaaS gateway</li> </ol> <p>First, you need to install the latest version of <code>das-cli</code> in your environment. Follow the instructions in the toolbox repo to make this.</p> <p>Then we'll start by setting up the environment.</p> <p>THE COMMANDS BELOW WILL CREATE FILES IN YOUR FILESYSTEM.</p> <p>Run the following cell.</p> <pre><code>!das-cli config list\n</code></pre> <p>If it outputs something like this:</p> <pre><code>+----------+----------------+-----------------------+\n| Service  | Name           | Value                 |\n+----------+----------------+-----------------------+\n| redis    | port           | 29000                 |\n| redis    | container_name | das-cli-redis-29000   |\n| mongodb  | port           | 28000                 |\n| mongodb  | container_name | das-cli-mongodb-28000 |\n| mongodb  | username       | dbadmin               |\n| mongodb  | password       | dassecret             |\n| loader   | container_name | das-cli-loader        |\n| openfaas | container_name | das-cli-openfaas-8080 |\n+----------+----------------+-----------------------+\n</code></pre> <p>It's because you already have a config file in <code>~/.das</code>. If that's the case you need to decide if you want to re-use the same port numbers or not. It's OK to have several databases in your machine. They are Docker containers listening in the given port.</p> <p>If the previous <code>das-cli config list</code> command output is empty, you just need to create a new config file. You can do so by running</p> <pre><code>das-cli config set\n</code></pre> <p>In a terminal. When you have done it, run the next cell to make sure you have a config file in place.</p> <pre><code>!das-cli config list\n</code></pre> <p>Containers for the DBMS servers and OpenFaas will be created listening on the given ports. Run the next cell to make sure any previously used containers are properly removed. If there are none, nothing will be done.</p> <pre><code>!das-cli db stop\n!das-cli faas stop\n</code></pre> <p>Now we need to start the DBMS servers.</p> <pre><code>!das-cli db start\n</code></pre> <p>You can double check that the DB containers are in place listing the active docker containers.</p> <pre><code>!docker ps\n</code></pre> <p>You should see containers for Redis and MongoDB listening on the ports you defined in the config file.</p> <p>Now we need to load a MeTTa file. You can use your own file here or run the next cell to download the same file we used in this section.</p> <pre><code>!wget -o /tmp/.get.output https://raw.githubusercontent.com/singnet/das-metta-parser/master/tests/data/animals.metta &amp;&amp; mv -f animals.metta /tmp\n</code></pre> <p>You may want to change the path in the cell below to point to another file.</p> <pre><code>!das-cli metta load /tmp/animals.metta\n</code></pre> <p>You may call <code>das-cli metta load</code> multiple times loading different files. To clear the databases you can use <code>das-cli db restart</code>.</p> <p>Once you're done loading the knowledge base, you need to start the FaaS server.</p> <pre><code>!das-cli faas start\n</code></pre> <p>It's done. At this point you should be able to point one or more remote DAS to this DAS Server, as we described here.</p>"},{"location":"developer_guidelines/","title":"Developer Guidelines","text":""},{"location":"developer_guidelines/#introduction","title":"Introduction","text":"<p>General guidelines and suggestions while working on DAS repositories.</p>"},{"location":"developer_guidelines/#pull-requests","title":"Pull Requests","text":"<p>Pull Requests (PRs) are crucial for collaboration in software development. A well-crafted PR title helps reviewers understand the purpose and scope of your changes quickly. This guide outlines best practices for creating clear and informative Pull Requests.</p> <p>While not currently enforced, we do want to link all PR's with it's own tracking issue at the DAS Board</p>"},{"location":"developer_guidelines/#guidelines-for-pr-titles","title":"Guidelines for PR Titles","text":"<ol> <li>Format: Use the format <code>[#issue] Concise, self-contained, explanatory title</code>.</li> <li>Include Issue References: Always reference the associated issue number in the PR title. This links the PR to the relevant discussion or task in our tasks dashboard.</li> <li> <p>Conciseness: Keep the title succinct while conveying the essence of the change. Avoid unnecessary details.</p> </li> <li> <p>Self-contained Explanation: Ensure the title provides enough context to understand the changes without needing to delve into the PR description immediately.</p> </li> <li> <p>Clarity and Specificity: Clearly describe what the PR accomplishes. Use specific terms related to functionality, bug fixes, or enhancements.</p> </li> <li> <p>Review Before Submitting: Double-check your title to ensure it accurately reflects the content and purpose of your changes.</p> </li> <li> <p>Think from the Reviewer's Perspective: Imagine you are reviewing the PR\u2014what information would you need from the title to understand the changes quickly?</p> </li> <li> <p>Examples:</p> <ul> <li><code>[#234] Refactor authentication service for improved performance</code></li> <li><code>[#890] Fix issue with date formatting in event calendar</code></li> </ul> </li> </ol> <p>Clear and informative PR titles help streamline the review process and ensure that changes are understood and integrated smoothly.</p>"},{"location":"developer_guidelines/#handling-epic-issues","title":"Handling EPIC Issues","text":"<p>To ensure proper management and tracking of EPIC issues on GitHub, follow these guidelines:</p> <ol> <li>Labeling EPIC Issues: Add the <code>EPIC</code> label whenever creating an epic    issue.</li> <li> <p>Mandatory Text in EPIC Issues: Include the following mandatory text in    the epic issue description:    ```    EPIC</p> </li> <li> <p></p> </li> <li></li> <li> <p>...    ```</p> </li> <li> <p>Here is an example of an epic issue: https://github.com/orgs/singnet/projects/6/views/1?filterQuery=epic+label%3AEPIC&amp;pane=issue&amp;itemId=66967931</p> </li> </ol>"},{"location":"developer_guidelines/#conclusion","title":"Conclusion","text":"<p>By following these guidelines, you can significantly improve communication and efficiency within our development team.</p>"},{"location":"release-notes/","title":"Release Notes","text":""},{"location":"release-notes/#das-version-020","title":"DAS Version 0.2.0","text":"<ul> <li>Toolbox das-cli: 0.3.0</li> <li>hyperon-das: 0.8.0</li> <li>hyperon-das-atomdb: 0.7.0</li> <li>FaaS functions: 1.13.0</li> <li>MeTTa Parser: 0.4.0</li> </ul>"},{"location":"release-notes/#changelog","title":"Changelog","text":""},{"location":"release-notes/#metta-parser-040","title":"MeTTa Parser 0.4.0","text":"<pre><code>[#qe231] Fixed bug in large-arity expressions\n</code></pre>"},{"location":"release-notes/#toolbox-das-cli-030","title":"Toolbox das-cli 0.3.0","text":"<pre><code>[#47] Add support to \":\" in symbol names\n[#51] Add minor features to das-cli and update documentation\n    das-cli --version\n    das-cli update-version [--version] (defaulted to newest version)\n    Rename das-cli server to das-cli db\n    das-cli db restart\n    das-cli faas restart\n    Remove parameter --path in das-cli metta load and das-cli metta validate and get the input file as a required parameter.\n    Rename das-cli metta validate to das-cli metta check.\n    das-cli server start should sleep for a couple of seconds after finishing the startup of DB containers\n    Show progress bar printed by the metta parser binaries (db_loader and syntax_check) when executing das-cli metta load and das-cli metta validate\n    Change message showing default version of the running function to show the actual version number.\n    Add a das-cli logs das to follow the DAS log das.log\n    Add das-cli jupyter-notebook start start a jupyter-notebook server running with all required dependencies to use hyperon-das.\n    Adjust runtime messages for das-cli example local and das-cli example faas. Both show python commands\n    das-cli faas update-version [--version]\n    das-cli faas --version\n    das-cli python-library version: show currently installed and newest available versions of both, hyperon-das and hyperon-das-atomdb\n    das-cli python-library update. Update hyperon-das to the newest available version. As a consequence, hyperon-das-atomdb should be updated to the proper required version as well.\n    das-cli python-library set --hyperon-das 0.4.0 --hyperon-das-atomdb 0.8.2 Allow setting specific versions for both libraries\n    das-cli python-library list by default, list all major/minor versions of hyperon-das and hyperon-das-atomdb. There should have optional parameters --show-patches and --library &lt;xxx&gt;\n    Add a new command to see release notes of specific version of specific package or lib. das-cli release-notes.\n[#62] Remove example python files for local/remote DAS in das-cli examples\n[#59] Fix das-cli --version output message\n[#43] Improve DAS CLI Manual\n[#69] Minor das-cli fixes\n    Updated das-cli metta check/load output to only display db_load and syntax_check outputs.\n    Modified das-cli faas update-version output to provide both old and new version information and advise callers to use das-cli faas restart to update a running faas server and also warns if no newer version is available.\n    Applied changes to the das-cli update-version command output to provide both old and new version information and also warns if no newer version is available.\n    Removed references to the example files distributed_atom_space_local.py and distributed_atom_space_remote.py, as well as the files themselves.\n    Reviewed \"Segmentation fault (core dumped)\" error\n    Added support for non-Ubuntu-based distributions to utilize the update-version command. Note that this command was developed and primarily tested on Ubuntu distributions.\n[#73] das-cli python-library version is raising an error\n[#76] Put version number in openfaas docker image name\n[#75] Enable the configuration of a Redis cluster instead of maintaining only a standalone instance\n[#87] Configuration of a Mongodb cluster instead of maintaining only a standalone instance\n</code></pre>"},{"location":"release-notes/#hyperon-das-atomdb-070","title":"hyperon-das-atomdb 0.7.0","text":"<pre><code>[#127] Create bulk_insert() in Adapters\n[#132] Fix bug in create_field_index() method\n[das-query-engine#223] Update log messages\n[#129] Create a new adapter called PostgresLobeDB\n[das-query-engine#214] Add retrieve_all_atoms method\n[#124] Changed count_atoms() to return more accurate numbers\n[das-query-engine#197] Changed get_all_links() to return a tuple\n[#142] Changed add_link() and add_node() to work with get_atom returns\n[das-query-engine#114] Changed commit() to receive buffer as a kwargs parameter\n[#63] Changed MongoFieldNames to FieldNames and placed it generally for all adapters\n[das#45] Round 1 - Initial refactoring of RAM Only DAS\n[#46] Add support for MongoDB indexes\n[#153] Refactoring create index\n</code></pre>"},{"location":"release-notes/#hyperon-das-080","title":"hyperon-das 0.8.0","text":"<pre><code>[#201] Implement fetch() in the DAS API\n[#223] Updates in docstrings and logging messages\n[#210] Improve message error when connecting to the server\n[#214] Improve fetch() method to optionally fetch all the atoms\n[#241] Fix tests\n[#213] Add TraverseEngine to API documentation\n[#114] Persist new atoms in remote server\n[#216] Changed design for custom filters in TraverseEngine\n[#229] Improve error handler\n[#218] Make a DAS server read-only\n[#223] Updated kwargs documentation\n[#268] Adding performance tests in MeTTa\n[#268] enhancing perf tests script, added results comparison\n[#276] API support to AtomDB new queries\n</code></pre>"},{"location":"release-notes/#faas-functions-1130","title":"FaaS functions 1.13.0","text":"<pre><code>[#100] Add fetch. New Action\n[#113] Add create_context command in query-engine function\n</code></pre>"},{"location":"release-notes/#das-version-010","title":"DAS Version 0.1.0","text":"<ul> <li>hyperon-das: 0.7.0</li> <li>hyperon-das-atomdb: 0.6.0</li> <li>FaaS functions: 1.12.0</li> <li>MeTTa Parser: 0.3.0</li> <li>Toolbox das-cli: 0.2.0</li> </ul>"},{"location":"release-notes/#changelog_1","title":"Changelog","text":""},{"location":"release-notes/#hyperon-das-070","title":"hyperon-das 0.7.0","text":"<pre><code>[#180] Fix in the test_metta_api.py integration test\n[#136] Implement methods in the DAS API to create indexes in the database\n[#BUGFIX] Fix Mock in unit tests\n[#90] OpenFaas is not serializing/deserializing query answers\n[#190] Implement custom_query() method in DAS API\n[#184] Fix bug that prevented DAS from answering nested queries properly\n[#202] Fix tests after adding complex typedef expressions\n[BUGFIX] Fix tests to compare dicts using only commom keys\n</code></pre>"},{"location":"release-notes/#hyperon-das-atomdb-060","title":"hyperon-das-atomdb 0.6.0","text":"<pre><code>[#112] Fix return of the functions get_matched_links(), get_incoming_links(), get_matched_type_template(), get_matched_type() from set to list\n[#114] Add create_field_index() to RedisMongoDB adapter\n[#120] Refactor Collections in RedisMongoDB adapter\n[#118] Create a new set in Redis to save custom index filters\n</code></pre>"},{"location":"release-notes/#faas-functions-1120","title":"FaaS functions 1.12.0","text":"<pre><code>[#57] Enable Running Integration Tests Locally with AtomDB Source Code and Local Query Engine\n[#62] Create integration tests for get_incoming_links\n[#64] Shutdown containers before run tests\n[#90] OpenFaas is not serializing/deserializing query answers\n[#97] Add custom_query() and update create_field_index()\n</code></pre>"},{"location":"release-notes/#metta-parser-030","title":"MeTTa Parser 0.3.0","text":"<pre><code>[#36] Add support for comments and escaped characters in literals and symbol names\n[#39] Add support for empty expressions\n[das-query-engine#192] Add support to complex type definitions in DAS MeTTa parser.\n[das-atom-db#120] Refactor collection in MongoDB\n[#43] Bugfix - BSON data being corrupted after implementation of complext typedef expressions\n</code></pre>"},{"location":"release-notes/#toolbox-das-cli-020","title":"Toolbox das-cli 0.2.0","text":"<pre><code>[#44] das-cli faas start should have optional parameters\n[#45] Fix DAS-CLI metta loader container\n[#41] Developing a DAS CLI Manual Using the 'man' Command\n[#30] Publish debian package to a package manager\n[#32] Optimizing user feedback and experience for scripts\n[#33] Remove poc loader\n[#28] Generate Debian package\n[#27] When you type 'server stop,' only shut down the server\n[das-metta-parser/#36] Add support for comments and escaped characters in literals and symbol names\nChange MeTTa parser version to 0.2.4\n</code></pre>"},{"location":"api/DAS/","title":"DAS","text":""},{"location":"api/DAS/#das.DistributedAtomSpace","title":"<code>DistributedAtomSpace</code>","text":""},{"location":"api/DAS/#das.DistributedAtomSpace.__init__","title":"<code>__init__(system_parameters={}, **kwargs)</code>","text":"<p>Creates a new DAS object. A DAS client can run locally or locally and remote, connecting to remote DASs instances to query remote atoms, if there're different versions of the same atom in local and one of the remote DASs, the local version is returned. When running along a remote DAS a host and port is mandatory, by default local instances of the DBs are created, remote instances can be configured using kwargs options.</p> <p>Parameters:</p> Name Type Description Default <code>system_parameters</code> <code>Dict[str, Any]</code> <p>Sets the system parameters. Defaults to { 'running_on_server': False, 'cache_enabled': False, 'attention_broker_hostname': 'localhost', 'attention_broker_port': 27000}.</p> <code>{}</code> <p>Other Parameters:</p> Name Type Description <code>atomdb</code> <code>str</code> <p>AtomDB type supported values are 'ram' and 'redis_mongo'. Defaults to 'ram'.</p> <code>query_engine</code> <code>str</code> <p>Set the type of connection for the query engine, values are 'remote' or 'local'. When this arg is set to 'remote', additional kwargs are required as host and port to connect to the remote query engine and the arg mode is used to configure the read/write privileges. Defaults to 'local'</p> <code>host</code> <code>str</code> <p>Sets the host for the remote query engine, it's mandatory when the query_engine is equal to 'remote'.</p> <code>port</code> <code>str</code> <p>Sets the port for the remote query engine, it's mandatory when the query_engine is equal to 'remote'.</p> <code>mode</code> <code>str</code> <p>Set query engine's ACL privileges, only available when the query_engine is set to 'remote', accepts 'read-only' or 'read-write'. Defaults to 'read-only'</p> <code>mongo_hostname</code> <code>str</code> <p>MongoDB's hostname, the local or remote query engine can connect to a remote server or run locally. Defaults to 'localhost'</p> <code>mongo_port</code> <code>int</code> <p>MongoDB port, set this arg if the port is not the standard. Defaults to 27017.</p> <code>mongo_username</code> <code>str</code> <p>Username used for authentication in the MongoDB database. Defaults to 'mongo'.</p> <code>mongo_password</code> <code>str</code> <p>Password used for authentication in the MongoDB database. Defaults to 'mongo'.</p> <code>mongo_tls_ca_file</code> <code>Any</code> <p>Full system path to the TLS certificate.</p> <code>redis_hostname</code> <code>str</code> <p>Redis hostname, the local or remote query engine can connect to a remote server or run locally. Defaults to 'localhost'</p> <code>redis_port</code> <code>int</code> <p>Redis port, set this arg if the port is not the standard. Defaults to 6379.</p> <code>redis_username</code> <code>str</code> <p>Username used for authentication in the Redis database, no credentials (username/password) are needed when running locally.</p> <code>redis_password</code> <code>str</code> <p>Password used for authentication in the Redis database.</p> <code>redis_cluster</code> <code>bool</code> <p>Indicates whether Redis is configured in cluster mode. Defaults to True.</p> <code>redis_ssl</code> <code>bool</code> <p>Set Redis to encrypt the connection. Defaults to True.</p>"},{"location":"api/DAS/#das.DistributedAtomSpace.add_link","title":"<code>add_link(link_params)</code>","text":"<p>Adds a link to DAS.</p> <p>A link is represented by a Python dict which may contain any number of keys associated to values of any type (including lists, sets, nested dicts, etc) , which are all recorded with the link, but must contain at least the keys \"type\" and \"targets\". \"type\" should map to a string and \"targets\" to a list of Python dict, each of them being itself a representation of either a node or a nested link. \"type\" and \"targets\" define the link uniquely, i.e. two links with the same \"type\" and \"targets\" are considered to be the same entity.</p> <p>Parameters:</p> Name Type Description Default <code>link_params</code> <code>Dict[str, Any]</code> <p>A dictionary with link data. The following keys are mandatory: - 'type': The type of the link. - 'targets': A list of target elements.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The information about the added link, including its unique handle and</p> <code>Dict[str, Any]</code> <p>other fields used internally in DAS.</p> <p>Raises:</p> Type Description <code>AddLinkException</code> <p>If the 'type' or 'targets' fields are missing or invalid somehow.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; das = DistributedAtomSpace()\n&gt;&gt;&gt; link_params = {\n        'type': 'Evaluation',\n        'targets': [\n            {'type': 'Predicate', 'name': 'Predicate:has_name'},\n            {\n                'type': 'Set',\n                'targets': [\n                    {'type': 'Reactome', 'name': 'Reactome:R-HSA-164843'},\n                    {'type': 'Concept', 'name': 'Concept:2-LTR circle formation'},\n                ],\n            },\n        ],\n    }\n&gt;&gt;&gt; das.add_link(link_params)\n</code></pre>"},{"location":"api/DAS/#das.DistributedAtomSpace.add_node","title":"<code>add_node(node_params)</code>","text":"<p>Adds a node to DAS.</p> <p>A node is represented by a Python dict which may contain any number of keys associated to values of any type (including lists, sets, nested dicts, etc) , which are all recorded with the node, but must contain at least the keys \"type\" and \"name\" mapping to strings which define the node uniquely, i.e. two nodes with the same \"type\" and \"name\" are considered to be the same entity.</p> <p>Parameters:</p> Name Type Description Default <code>node_params</code> <code>Dict[str, Any]</code> <p>A dictionary with node data. The following keys are mandatory: - 'type': Node type - 'name': Node name</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The information about the added node, including its unique handle and</p> <code>Dict[str, Any]</code> <p>other fields used internally in DAS.</p> <p>Raises:</p> Type Description <code>AddNodeException</code> <p>If 'type' or 'name' fields are missing or invalid somehow.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; das = DistributedAtomSpace()\n&gt;&gt;&gt; node_params = {\n        'type': 'Reactome',\n        'name': 'Reactome:R-HSA-164843',\n    }\n&gt;&gt;&gt; das.add_node(node_params)\n</code></pre>"},{"location":"api/DAS/#das.DistributedAtomSpace.clear","title":"<code>clear()</code>","text":"<p>Delete all atoms and custom indexes.</p>"},{"location":"api/DAS/#das.DistributedAtomSpace.commit_changes","title":"<code>commit_changes(**kwargs)</code>","text":"<p>Commit changes (atom addition/deletion/change) to the databases or to the remote DAS Server, depending on the type of DAS being used.</p> <p>The behavior of this method depends on the type of DAS being used.</p> <ol> <li> <p>When called in a DAS instantiated with query_engine=remote</p> <p>This is called a \"Remote DAS\" in the documentation. Remote DAS is connected to a remote DAS Server which is used to make queries, traversing, etc but it also keeps a local Atomspace in RAM which is used as a cache. Atom changes are made initially in this local cache. When commit_changes() is called in this type of DAS, these changes are propagated to the remote DAS Server.</p> </li> <li> <p>When called in a DAS instantiated with query_engine=local and    atomdb='ram'.</p> <p>No effect.</p> </li> <li> <p>When called in a DAS instantiated with query_engine=local and    atomdb='redis_mongo'</p> <p>The AtomDB keeps buffers of changes which are not actually written in the DBs until commit_changes() is called (or until that buffers size reach a threshold).</p> </li> </ol>"},{"location":"api/DAS/#das.DistributedAtomSpace.count_atoms","title":"<code>count_atoms(parameters=None)</code>","text":"<p>Count atoms, nodes and links in DAS.</p> <p>By default, the precise parameter is set to False returning the total number of atoms, without node and link counts. If the precise parameter is True it will return the total of nodes and links and atoms.</p> <p>In the case of remote DAS, count the total number of nodes and links stored locally and remotely. If there are more than one instance of the same atom (local and remote), it's counted only once.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Optional[Dict[str, Any]]</code> <p>Dict containing the following keys: 'context' - returning the count of 'local', 'remote' or 'both', 'precise' - boolean if True provides an accurate but slower count, if False the count will be an estimate, which is faster but less precise. Default value for 'context' is 'both' and for 'precise' is False. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dict[str, int]: Dict containing the keys 'node_count', 'atom_count', 'link_count'.</p>"},{"location":"api/DAS/#das.DistributedAtomSpace.create_field_index","title":"<code>create_field_index(atom_type, fields, named_type=None, composite_type=None, index_type=None)</code>","text":"<p>Create a custom index on the passed field of all atoms of the passed type.</p> <p>Remote DAS allow creation of custom indexes based on custom fields in nodes or links. These indexes can be used to make subsequent custom queries.</p> <p>Parameters:</p> Name Type Description Default <code>atom_type</code> <code>str</code> <p>Either 'link' or 'node', if the index is to be created for links or nodes.</p> required <code>fields</code> <code>List[str]</code> <p>fields where the index will be created upon</p> required <code>named_type</code> <code>str</code> <p>Only atoms of the passed type will be indexed. Defaults to None, meaning that atom type doesn't matter.</p> <code>None</code> <code>composite_type</code> <code>List[Any]</code> <p>Only Atoms type of the passed composite type will be indexed. Defaults to None.</p> <code>None</code> <code>index_type</code> <code>Optional[str]</code> <p>Type of index, values allowed are 'binary_tree' to create indexes using binary tree in ascending order or 'token_inverted_list' to create index for text field search the text field will be tokenized and every token will be an indexed. Only one token_inverted_list field is allowed. If set as None will create a binary tree index. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid somehow.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The index ID. This ID should be used to make subsequent queries using this newly created index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; index_id = das.create_field_index('link', ['tag'], type='Expression')\n&gt;&gt;&gt; index_id = das.create_field_index('link', ['tag'], composite_type=['Expression', 'Symbol', 'Symbol', ['Expression', 'Symbol', 'Symbol', 'Symbol']])\n</code></pre>"},{"location":"api/DAS/#das.DistributedAtomSpace.custom_query","title":"<code>custom_query(index_id, query, **kwargs)</code>","text":"<p>Perform a query using a previously created custom index.</p> <p>Actual query parameters can be passed as kwargs according to the type of the previously created filter.</p> <p>Parameters:</p> Name Type Description Default <code>index_id</code> <code>str</code> <p>custom index id to be used in the query.</p> required <code>query</code> <code>Dict[str, Any]</code> <p>Query dict, fields are the dict's keys and values are the search. It supports multiple fields. eg: {'name': 'human'}</p> required <p>Other Parameters:</p> Name Type Description <code>no_iterator</code> <code>bool</code> <p>Set False to return an iterator otherwise it will return a list of Dict[str, Any]. If the query_engine is set to 'remote' it always return an iterator. Defaults to True.</p> <code>cursor</code> <code>Any</code> <p>Cursor position in the iterator, starts retrieving links from redis at the cursor position. Defaults to 0.</p> <code>chunk_size</code> <code>int</code> <p>Chunk size. Defaults to 1000.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If called from Local DAS in RAM only.</p> <p>Returns:</p> Type Description <code>Union[Iterator, List[Dict[str, Any]]]</code> <p>Union[Iterator, List[Dict[str, Any]]]: An iterator or list of dict containing atom data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; das.custom_query(index_id='index_123', query={'tag': 'DAS'})\n&gt;&gt;&gt; das.custom_query(index_id='index_123', query={'tag': 'DAS'}, no_iterator=True)\n</code></pre>"},{"location":"api/DAS/#das.DistributedAtomSpace.fetch","title":"<code>fetch(query=None, host=None, port=None, **kwargs)</code>","text":"<p>Fetch, from a DAS Server, all atoms that match the passed query or all atoms in the server if None is passed as query.</p> <p>Instead of adding atoms by calling add_node() and add_link() directly, it's possible to fetch all or part of the contents from a DAS server using the method fetch(). This method doesn't create a lasting connection with the DAS server, it will just fetch the atoms once and close the connection so any subsequent changes or queries will not be propagated to the server in any way. After fetching the atoms, all queries will be made locally. It's possible to call fetch() multiple times fetching from the same DAS Server or from different ones.</p> <p>The input query is a link, used as a pattern to make the query. Variables can be used as link targets as well as nodes. Nested links are allowed as well.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Optional[Union[List[dict], dict]]</code> <p>A pattern described as a link (possibly with nested links) with nodes and variables used to query the knowledge base. Defaults to None</p> <code>None</code> <code>host</code> <code>Optional[str]</code> <p>Address to remote server. Defaults to None.</p> <code>None</code> <code>port</code> <code>Optional[int]</code> <p>Port to remote server. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters ar somehow invalid.</p> <p>Returns:</p> Type Description <code>Union[None, List[dict]]</code> <p>Union[None, List[dict]]: Returns None.</p> <code>Union[None, List[dict]]</code> <p>If runing on the server returns a list of dictionaries containing detailed information of the atoms.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; query = {\n        \"atom_type\": \"link\",\n        \"type\": \"Expression\",\n        \"targets\": [\n            {\"atom_type\": \"node\", \"type\": \"Symbol\", \"name\": \"Inheritance\"},\n            {\"atom_type\": \"variable\", \"name\": \"v1\"},\n            {\"atom_type\": \"node\", \"type\": \"Symbol\", \"name\": '\"mammal\"'},\n        ],\n    }\n    das = DistributedAtomSpace()\n    das.fetch(query, host='123.4.5.6', port=8080)\n</code></pre>"},{"location":"api/DAS/#das.DistributedAtomSpace.get_atom","title":"<code>get_atom(handle, **kwargs)</code>","text":"<p>Retrieve an atom given its handle, handles for atoms can be created by using the function 'get_node_handle'. A handle is MD5 hash of a node in the graph.</p> <p>Parameters:</p> Name Type Description Default <code>handle</code> <code>str</code> <p>Atom's handle.</p> required <p>Other Parameters:</p> Name Type Description <code>no_target_format</code> <code>bool</code> <p>Set this parameter to True to get MongoDB's default output. Defaults to False.</p> <code>targets_document</code> <code>bool</code> <p>Set this parameter to True to return a tuple containing the document as first element and the targets as second element. Defaults to False.</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict[str, Any]</code> <p>A Python dict with all atom data.</p> <p>Raises:</p> Type Description <code>AtomDoesNotExist</code> <p>If the corresponding atom doesn't exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; das = DistributedAtomSpace()\n&gt;&gt;&gt; human_handle = das.get_node_handle(node_type='Concept', node_name='human')\n&gt;&gt;&gt; result = das.get_atom(human_handle)\n&gt;&gt;&gt; print(result)\n{\n    'handle': 'af12f10f9ae2002a1607ba0b47ba8407',\n    'composite_type_hash': 'd99a604c79ce3c2e76a2f43488d5d4c3',\n    'name': 'human',\n    'named_type': 'Concept'\n}\n</code></pre>"},{"location":"api/DAS/#das.DistributedAtomSpace.get_atoms_by_field","title":"<code>get_atoms_by_field(query)</code>","text":"<p>Search for the atoms containing field and value, performance is improved if an index was previously created.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Dict[str, Any]</code> <p>Query dict, fields are the dict keys and values are the search. It supports multiple fields. eg: {'name': 'human'}</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of atom's ids</p>"},{"location":"api/DAS/#das.DistributedAtomSpace.get_atoms_by_text_field","title":"<code>get_atoms_by_text_field(text_value, field=None, text_index_id=None)</code>","text":"<p>Performs a text search, if a text index is previously created performance a token index search, otherwise will perform a regex search using binary tree and the argument 'field' is mandatory. Performance is improved if a 'binary_tree' or 'token_inverted_list' is previously created using 'create_field_index' method.</p> <p>Parameters:</p> Name Type Description Default <code>text_value</code> <code>str</code> <p>Text value to search for</p> required <code>field</code> <code>Optional[str]</code> <p>Field to check the text_value</p> <code>None</code> <code>text_index_id</code> <code>Optional[str]</code> <p>Text index</p> <code>None</code> <p>Returns:     List[str]: List of atom's ids</p>"},{"location":"api/DAS/#das.DistributedAtomSpace.get_incoming_links","title":"<code>get_incoming_links(atom_handle, **kwargs)</code>","text":"<p>Retrieve all links which has the passed handle as one of its targets.</p> <p>Parameters:</p> Name Type Description Default <code>atom_handle</code> <code>str</code> <p>Atom's handle</p> required <p>Other Parameters:</p> Name Type Description <code>no_iterator</code> <code>bool</code> <p>Set False to return an iterator otherwise it will return a list of Dict[str, Any]. If the query_engine is set to 'remote' it always return an iterator. Defaults to True.</p> <code>cursor</code> <code>int</code> <p>Cursor position in the iterator, starts retrieving links from redis at the cursor position. Defaults to 0.</p> <code>handles_only</code> <code>bool</code> <p>Returns a list of links handles.</p> <p>Returns:</p> Type Description <code>List[Union[Dict[str, Any], str]]</code> <p>List[Dict[str, Any]]: A list of dictionaries containing detailed information of the atoms</p> <code>List[Union[Dict[str, Any], str]]</code> <p>or a list of strings containing the atom handles</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; das = DistributedAtomSpace()\n&gt;&gt;&gt; rhino = das.get_node_handle('Concept', 'rhino')\n&gt;&gt;&gt; links = das.get_incoming_links(rhino)\n&gt;&gt;&gt; for link in links:\n&gt;&gt;&gt;     print(link['type'], link['targets'])\nSimilarity ['d03e59654221c1e8fcda404fd5c8d6cb', '99d18c702e813b07260baf577c60c455']\nSimilarity ['99d18c702e813b07260baf577c60c455', 'd03e59654221c1e8fcda404fd5c8d6cb']\nInheritance ['99d18c702e813b07260baf577c60c455', 'bdfe4e7a431f73386f37c6448afe5840']\n</code></pre>"},{"location":"api/DAS/#das.DistributedAtomSpace.get_link","title":"<code>get_link(link_type, link_targets)</code>","text":"<p>Retrieve a link given its type and list of targets. Targets are hashes of the nodes these hashes or handles can be created using the function 'get_node_handle'.</p> <p>Parameters:</p> Name Type Description Default <code>link_type</code> <code>str</code> <p>Link type</p> required <code>link_targets</code> <code>List[str]</code> <p>List of target handles.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict[str, Any]</code> <p>A Python dict with all link data.</p> <p>Raises:</p> Type Description <code>AtomDoesNotExist</code> <p>If the corresponding link doesn't exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; das = DistributedAtomSpace()\n&gt;&gt;&gt; human_handle = das.get_node_handle('Concept', 'human')\n&gt;&gt;&gt; monkey_handle = das.get_node_handle('Concept', 'monkey')\n&gt;&gt;&gt; result = das.get_link(\n        link_type='Similarity',\n        link_targets=[human_handle, monkey_handle],\n    )\n&gt;&gt;&gt; print(result)\n{\n    'handle': 'bad7472f41a0e7d601ca294eb4607c3a',\n    'composite_type_hash': 'ed73ea081d170e1d89fc950820ce1cee',\n    'is_toplevel': True,\n    'composite_type': [\n        'a9dea78180588431ec64d6bc4872fdbc',\n        'd99a604c79ce3c2e76a2f43488d5d4c3',\n        'd99a604c79ce3c2e76a2f43488d5d4c3'\n    ],\n    'named_type': 'Similarity',\n    'named_type_hash': 'a9dea78180588431ec64d6bc4872fdbc',\n    'targets': [\n        'af12f10f9ae2002a1607ba0b47ba8407',\n        '1cdffc6b0b89ff41d68bec237481d1e1'\n    ]\n}\n</code></pre>"},{"location":"api/DAS/#das.DistributedAtomSpace.get_link_handle","title":"<code>get_link_handle(link_type, link_targets)</code>  <code>staticmethod</code>","text":"<p>Computes the handle of a link, given its type and targets' handles.</p> <p>Note that this is a static method which don't actually query the stored atomspace in order to compute the handle. Instead, it just run a MD5 hashing algorithm on the parameters that uniquely identify links (i.e. type and list of targets) This means e.g. that two links with the same type and the same targets are considered to be the exact same entity as they will have the same handle.</p> <p>Parameters:</p> Name Type Description Default <code>link_type</code> <code>str</code> <p>Link type.</p> required <code>link_targets</code> <code>List[str]</code> <p>List with the target handles.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Link's handle.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; das = DistributedAtomSpace()\n&gt;&gt;&gt; human_handle = das.get_node_handle(node_type='Concept', node_name='human')\n&gt;&gt;&gt; monkey_handle = das.get_node_handle(node_type='Concept', node_name='monkey')\n&gt;&gt;&gt; result = das.get_link_handle(link_type='Similarity', targets=[human_handle, monkey_handle])\n&gt;&gt;&gt; print(result)\n\"bad7472f41a0e7d601ca294eb4607c3a\"\n</code></pre>"},{"location":"api/DAS/#das.DistributedAtomSpace.get_links","title":"<code>get_links(link_type, target_types=None, link_targets=None, **kwargs)</code>","text":"<p>Retrieve all links that match the passed search criteria.</p> <p>This method can be used in four different ways.</p> <ol> <li> <p>Retrieve all the links of a given type</p> <p>Set link_type to the desired type and set target_types=None and link_targets=None.</p> </li> <li> <p>Retrieve all the links of a given type whose targets are of given types.</p> <p>Set link_type to the desired type and target_types to a list with the desired types os each target.</p> </li> <li> <p>Retrieve all the links of a given type whose targets match a given list of    handles</p> <p>Set link_type to the desired type (or pass link_type='' to retrieve links of any type) and set link_targets to a list of handles. Any handle in this list can be '' meaning that any handle in that position of the targets list is a match for the query. Set target_types=None.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>link_type</code> <code>str</code> <p>Link type being searched (can be '*' when link_targets is not None).</p> required <code>target_types</code> <code>List[str]</code> <p>Template of target types being searched.</p> <code>None</code> <code>link_targets</code> <code>List[str]</code> <p>Template of targets being searched (handles or '*').</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>no_iterator</code> <code>bool</code> <p>Set False to return an iterator otherwise it will return a list of Dict[str, Any]. If the query_engine is set to 'local' it always return an iterator. Defaults to True.</p> <code>cursor</code> <code>int</code> <p>Cursor position in the iterator, starts retrieving links from redis at the cursor position. Defaults to 0.</p> <code>chunk_size</code> <code>int</code> <p>Chunk size. Defaults to 1000.</p> <code>top_level_only</code> <code>bool optional</code> <p>Set to True to filter top level links. Defaults to False.</p> <p>Returns:</p> Type Description <code>Union[Iterator, List[Dict[str, Any]]]</code> <p>Union[Iterator, List[Dict[str, Any]]]: A list of dictionaries containing detailed</p> <code>Union[Iterator, List[Dict[str, Any]]]</code> <p>information of the links</p> <p>Examples:</p> <pre><code>1. Retrieve all the links of a given type\n    &gt;&gt;&gt; das = DistributedAtomSpace()\n    &gt;&gt;&gt; links = das.get_links(link_type='Inheritance')\n    &gt;&gt;&gt; for link in links:\n    &gt;&gt;&gt;     print(link['type'], link['targets'])\n    Inheritance ['5b34c54bee150c04f9fa584b899dc030', 'bdfe4e7a431f73386f37c6448afe5840']\n    Inheritance ['b94941d8cd1c0ee4ad3dd3dcab52b964', '80aff30094874e75028033a38ce677bb']\n    Inheritance ['bb34ce95f161a6b37ff54b3d4c817857', '0a32b476852eeb954979b87f5f6cb7af']\n    ...\n\n2. Retrieve all the links of a given type whose targets are of given types.\n\n    &gt;&gt;&gt; links = das.get_links(link_type='Inheritance', target_types=['Concept', 'Concept'])\n    &gt;&gt;&gt; for link in links:\n    &gt;&gt;&gt;     print(link['type'], link['targets'])\n    Inheritance ['5b34c54bee150c04f9fa584b899dc030', 'bdfe4e7a431f73386f37c6448afe5840']\n    Inheritance ['b94941d8cd1c0ee4ad3dd3dcab52b964', '80aff30094874e75028033a38ce677bb']\n    Inheritance ['bb34ce95f161a6b37ff54b3d4c817857', '0a32b476852eeb954979b87f5f6cb7af']\n    ...\n\n3. Retrieve all the links of a given type whose targets match a given list of\n   handles\n\n    &gt;&gt;&gt; snake = das.get_node_handle('Concept', 'snake')\n    &gt;&gt;&gt; links = das.get_links(link_type='Similarity', link_targets=[snake, '*'])\n    &gt;&gt;&gt; for link in links:\n    &gt;&gt;&gt;     print(link['type'], link['targets'])\n    Similarity ['c1db9b517073e51eb7ef6fed608ec204', 'b94941d8cd1c0ee4ad3dd3dcab52b964']\n    Similarity ['c1db9b517073e51eb7ef6fed608ec204', 'bb34ce95f161a6b37ff54b3d4c817857']\n</code></pre>"},{"location":"api/DAS/#das.DistributedAtomSpace.get_node","title":"<code>get_node(node_type, node_name)</code>","text":"<p>Retrieve a node given its type and name.</p> <p>Args:self.query_engine     node_type (str): Node type     node_name (str): Node name</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict[str, Any]</code> <p>A Python dict with all node data.</p> <p>Raises:</p> Type Description <code>AtomDoesNotExist</code> <p>If the corresponding node doesn't exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; das = DistributedAtomSpace()\n&gt;&gt;&gt; result = das.get_node(\n        node_type='Concept',\n        node_name='human'\n    )\n&gt;&gt;&gt; print(result)\n{\n    'handle': 'af12f10f9ae2002a1607ba0b47ba8407',\n    'composite_type_hash': 'd99a604c79ce3c2e76a2f43488d5d4c3',\n    'name': 'human',\n    'named_type': 'Concept'\n}\n</code></pre>"},{"location":"api/DAS/#das.DistributedAtomSpace.get_node_by_name_starting_with","title":"<code>get_node_by_name_starting_with(node_type, startswith)</code>","text":"<p>Performs a search in the nodes names searchin for a node starting with the 'startswith' value.</p> <p>Parameters:</p> Name Type Description Default <code>node_type</code> <code>str</code> <p>Node type</p> required <code>startswith</code> <code>str</code> <p>String to search for</p> required <p>Returns:     List[str]: List of atom's ids</p>"},{"location":"api/DAS/#das.DistributedAtomSpace.get_node_handle","title":"<code>get_node_handle(node_type, node_name)</code>  <code>staticmethod</code>","text":"<p>Computes the handle of a node, given its type and name.</p> <p>Note that this is a static method which don't actually query the stored atomspace in order to compute the handle. Instead, it just run a MD5 hashing algorithm on the parameters that uniquely identify nodes (i.e. type and name) This means e.g. that two nodes with the same type and the same name are considered to be the exact same entity as they will have the same handle.</p> <p>Parameters:</p> Name Type Description Default <code>node_type</code> <code>str</code> <p>Node type</p> required <code>node_name</code> <code>str</code> <p>Node name</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Node's handle</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; das = DistributedAtomSpace()\n&gt;&gt;&gt; result = das.get_node_handle(node_type='Concept', node_name='human')\n&gt;&gt;&gt; print(result)\n\"af12f10f9ae2002a1607ba0b47ba8407\"\n</code></pre>"},{"location":"api/DAS/#das.DistributedAtomSpace.get_traversal_cursor","title":"<code>get_traversal_cursor(handle, **kwargs)</code>","text":"<p>Create and return a Traverse Engine, an object that can be used to traverse the atomspace hypergraph.</p> <p>A TraverseEngine is like a cursor which points to an atom in the hypergraph and can be used to probe for links and neighboring atoms and then move on by following links. It's functioning is closely tied to the cache system in order to optimize the order in which atoms are presented to the caller when probing the neighborhood and to use cache's \"atom paging\" capabilities to minimize latency when used in remote DAS.</p> <p>Parameters:</p> Name Type Description Default <code>handle</code> <code>str</code> <p>Atom's handle</p> required <p>Raises:</p> Type Description <code>GetTraversalCursorException</code> <p>If passed handle is invalid, somehow (e.g. if</p> <p>Returns:</p> Name Type Description <code>TraverseEngine</code> <code>TraverseEngine</code> <p>The object that allows traversal of the hypergraph.</p>"},{"location":"api/DAS/#das.DistributedAtomSpace.query","title":"<code>query(query, parameters={})</code>","text":"<p>Perform a query on the knowledge base using a dict as input and return an iterator of QueryAnswer objects. Each such object carries the resulting mapping of variables in the query and the corresponding subgraph which is the result of applying such mapping to rewrite the query.</p> <p>The input dict is a link, used as a pattern to make the query. Variables can be used as link targets as well as nodes. Nested links are allowed as well.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[List[Dict[str, Any]], Dict[str, Any]]</code> <p>A pattern described as a link (possibly with nested links) with nodes and variables used to query the knowledge base. If the query is represented as a list of dictionaries, it is interpreted as a conjunction (AND) of all queries within the list.</p> required <code>parameters</code> <code>Dict[str, Any]</code> <p>query optional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Iterator[QueryAnswer], List[QueryAnswer]]</code> <p>Iterator[QueryAnswer]: An iterator of QueryAnswer objects, which have a field 'assignment', with a mapping from variables to handles and another field 'subgraph', with the resulting subgraph after applying 'assignment' to rewrite the query.</p> <p>Raises:</p> Type Description <code>UnexpectedQueryFormat</code> <p>If query resolution lead to an invalid state</p> Notes <ul> <li>Logical connectors OR and NOT are not implemented yet.</li> <li>If no match is found for the query, an empty list is returned.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; das.add_link({\n    \"type\": \"Expression\",\n    \"targets\": [\n        {\"type\": \"Symbol\", \"name\": \"Test\"},\n        {\n            \"type\": \"Expression\",\n            \"targets\": [\n                {\"type\": \"Symbol\", \"name\": \"Test\"},\n                {\"type\": \"Symbol\", \"name\": \"2\"}\n            ]\n        }\n    ]\n})\n&gt;&gt;&gt; query_params = {\"toplevel_only\": False}\n&gt;&gt;&gt; q1 = {\n    \"atom_type\": \"link\",\n    \"type\": \"Expression\",\n    \"targets\": [\n        {\"atom_type\": \"variable\", \"name\": \"v1\"},\n        {\n            \"atom_type\": \"link\",\n            \"type\": \"Expression\",\n            \"targets\": [\n                {\"atom_type\": \"variable\", \"name\": \"v2\"},\n                {\"atom_type\": \"node\", \"type\": \"Symbol\", \"name\": \"2\"},\n            ]\n        }\n    ]\n}\n&gt;&gt;&gt; for result in das.query(q1, query_params):\n&gt;&gt;&gt;     print(result.assignment.mapping['v1'])\n&gt;&gt;&gt;     print(result.assignment.mapping['v2'])\n&gt;&gt;&gt;     print(result.assignment.subgraph)\n'233d9a6da7d49d4164d863569e9ab7b6'\n'963d66edfb77236054125e3eb866c8b5'\n[\n    {\n        'handle': 'dbcf1c7b610a5adea335bf08f6509978',\n        'type': 'Expression',\n        'template': ['Expression', 'Symbol', ['Expression', 'Symbol', 'Symbol']],\n        'targets': [\n            {'handle': '963d66edfb77236054125e3eb866c8b5', 'type': 'Symbol', 'name': 'Test'},\n            {\n                'handle': '233d9a6da7d49d4164d863569e9ab7b6',\n                'type': 'Expression',\n                'template': ['Expression', 'Symbol', 'Symbol'],\n                'targets': [\n                    {'handle': '963d66edfb77236054125e3eb866c8b5', 'type': 'Symbol', 'name': 'Test'},\n                    {'handle': '9f27a331633c8bc3c49435ffabb9110e', 'type': 'Symbol', 'name': '2'}\n                ]\n            }\n        ]\n    }\n]\n</code></pre>"},{"location":"api/DAS/#das.DistributedAtomSpace.reindex","title":"<code>reindex(pattern_index_templates=None)</code>","text":"<p>Rebuild all indexes according to the passed specification</p> <p>Parameters:</p> Name Type Description Default <code>pattern_index_templates</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>indexes are specified by atom type in a dict mapping from atom types to a pattern template:</p> <p>{     :  } <p>Pattern template is also a dict:</p> <p>{     \"named_type\": True/False     \"selected_positions\": [n1, n2, ...] }</p> <p>Pattern templates are applied to each link entered in the atom space in order to determine which entries should be created in the inverted pattern index. Entries in the inverted pattern index are like patterns where the link type and each of its targets may be replaced by wildcards. For instance, given a similarity link Similarity(handle1, handle2) it could be used to create any of the following entries in the inverted pattern index:</p> <pre><code>*(handle1, handle2)\nSimilarity(*, handle2)\nSimilarity(handle1, *)\nSimilarity(*, *)\n</code></pre> <p>If we create all possibilities of index entries to all links, the pattern index size will grow exponentially so we limit the entries we want to create by each type of link. This is what a pattern template for a given link type is. For instance if we apply this pattern template:</p> <p>{     \"named_type\": False     \"selected_positions\": [0, 1] }</p> <p>to Similarity(handle1, handle2) we'll create only the following entries:</p> <pre><code>Similarity(*, handle2)\nSimilarity(handle1, *)\nSimilarity(*, *)\n</code></pre> <p>If we apply this pattern template instead:</p> <p>{     \"named_type\": True     \"selected_positions\": [1] }</p> <p>We'll have:</p> <pre><code>*(handle1, handle2)\nSimilarity(handle1, *)\n</code></pre> <code>None</code>"},{"location":"api/Traverse%20Engine/","title":"Traverse Engine","text":""},{"location":"api/Traverse%20Engine/#traverse_engines.TraverseEngine","title":"<code>TraverseEngine</code>","text":""},{"location":"api/Traverse%20Engine/#traverse_engines.TraverseEngine.follow_link","title":"<code>follow_link(**kwargs)</code>","text":"<p>Update the current cursor by following the first of the neighbors that points to the current cursor.    Possible use cases to filter parameter:     a. traverse.get_neighbors(..., filter=custom_filter)         -&gt; The custom_filter will be applied to Links     b. traverse.get_neighbors(..., filter=(custom_filter1, custom_filter2))         -&gt; The custom_filter1 will be applied to Links and custom_filter2 will be applied to Targets     c. traverse.get_neighbors(..., filter=(None, custom_filter2))         -&gt; The custom_filter2 will only be applied to Targets. This way there is no filter to Links     d. traverse.get_neighbors(..., filter=(custom_filter1, None))         -&gt; The custom_filter1 will be applied to Links. This case is equal case <code>a</code></p> <p>Other Parameters:</p> Name Type Description <code>link_type</code> <code>str</code> <p>Filter links if named_type matches with this parameter.</p> <code>cursor_position</code> <code>int</code> <p>Sets the position of the cursor, return the links after this position.</p> <code>target_type</code> <code>str</code> <p>Filter links if one of the targets matches with this parameter.</p> <code>filter</code> <code>tuple(Callable[[Dict], bool], Callable[[Dict], bool])</code> <p>Tuple containing filter function for links at pos 0 and filter function for targets at pos 1. Used to filter the results after applying all other filters.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The current cursor. A Python dict with all atom data.</p>"},{"location":"api/Traverse%20Engine/#traverse_engines.TraverseEngine.get","title":"<code>get()</code>","text":"<p>Returns the current cursor.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The current cursor. A Python dict with all atom data.</p>"},{"location":"api/Traverse%20Engine/#traverse_engines.TraverseEngine.get_links","title":"<code>get_links(**kwargs)</code>","text":"<p>Returns all links that have the current cursor as one of their targets, that is, any links that point to the cursor.</p> <p>Other Parameters:</p> Name Type Description <code>link_type</code> <code>str</code> <p>Filter links if named_type matches with this parameter.</p> <code>cursor_position</code> <code>int</code> <p>Sets the position of the cursor, return the links after this position.</p> <code>target_type</code> <code>str</code> <p>Filter links if one of the targets matches with this parameter.</p> <code>filter</code> <code>Callable[[Dict], bool]</code> <p>Function used to filter the results after applying all other filters.</p> <code>chunk_size</code> <code>int</code> <p>Chunk size. Defaults to 500.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>An iterator that contains the links that match the criteria.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def has_score(atom):\n        if 'score' in atom and score &gt; 0.5:\n            return True\n        return False\n&gt;&gt;&gt; links = traverse_engine.get_links(\n        link_type='Ex',\n        cursor_position=2,\n        target_type='Sy',\n        filter=has_score\n    )\n&gt;&gt;&gt; next(links)\n</code></pre>"},{"location":"api/Traverse%20Engine/#traverse_engines.TraverseEngine.get_neighbors","title":"<code>get_neighbors(**kwargs)</code>","text":"<p>Get all of \"neighbors\" that pointing to current cursor.    Possible use cases to filter parameter:     a. traverse.get_neighbors(..., filter=custom_filter)         -&gt; The custom_filter will be applied to Links     b. traverse.get_neighbors(..., filter=(custom_filter1, custom_filter2))         -&gt; The custom_filter1 will be applied to Links and custom_filter2 will be applied to Targets     c. traverse.get_neighbors(..., filter=(None, custom_filter2))         -&gt; The custom_filter2 will only be applied to Targets. This way there is no filter to Links     d. traverse.get_neighbors(..., filter=(custom_filter1, None))         -&gt; The custom_filter1 will be applied to Links. This case is equal case <code>a</code></p> <p>Other Parameters:</p> Name Type Description <code>link_type</code> <code>str</code> <p>Filter links if named_type matches with this parameter.</p> <code>cursor_position</code> <code>int</code> <p>Sets the position of the cursor, return the links after this position.</p> <code>target_type</code> <code>str</code> <p>Filter links if one of the targets matches with this parameter.</p> <code>filter</code> <code>tuple(Callable[[Dict], bool], Callable[[Dict], bool])</code> <p>Tuple containing filter function for links at pos 0 and filter function for targets at pos 1. Used to filter the results after applying all other filters.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <code>Iterator</code> <p>An iterator that contains the neighbors that match the criteria.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; neighbors = traverse_engine.get_neighbors(\n        link_type='Ex',\n        cursor_position=2,\n        target_type='Sy',\n        filter=(link_filter, target_filter)\n    )\n&gt;&gt;&gt; next(neighbors)\n</code></pre>"},{"location":"api/Traverse%20Engine/#traverse_engines.TraverseEngine.goto","title":"<code>goto(handle)</code>","text":"<p>Reset current cursor to the passed handle.</p> <p>Parameters:</p> Name Type Description Default <code>handle</code> <code>str</code> <p>The handle of the atom to go to.</p> required <p>Raises:</p> Type Description <code>AtomDoesNotExist</code> <p>If the corresponding atom doesn't exist.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The current cursor. A Python dict with all atom data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; traverse_engine.goto('asd1234567890')\n&gt;&gt;&gt; {\n        'handle': 'asd1234567890',\n        'type': 'AI,\n        'composite_type_hash': 'd99asd1234567890',\n        'name': 'snet',\n        'named_type': 'AI'\n    }\n</code></pre>"}]}